# ğŸ“ Proje DÃ¶kÃ¼mÃ¼: coder-asistan

Bu dÃ¶kÃ¼m, **/home/ahmetc/proje/coder-asistan** dizini (mevcut klasÃ¶r) ve altÄ±ndakileri kapsar.

### ğŸ“‚ Proje Dizin YapÄ±sÄ± ve Dosyalar

- **coder-asistan/** (Proje KÃ¶kÃ¼)
  - assistant.py
  - readme.md
  - **core/**
    - base.py
    - gemini.py
    - huggingface.py
  - **src/**
    - app.py
    - requirements.txt
    - **handlers/**
      - user.py
  - **gemini_venv/**
    - pyvenv.cfg
  - **.gassist_backups/**
    - app.py.20251214_225803.backup

---
### ğŸ’» Kod Ä°Ã§eriÄŸi DÃ¶kÃ¼mÃ¼


#### ğŸ“„ Dosya: `assistant.py`

```py
# assistant.py (MODÃœLER Ã‡EKÄ°RDEK)
import sys
import os
import re
import json
import shutil
import glob
from datetime import datetime

# ModÃ¼l importlarÄ± (Gemini ve diÄŸer modelleri buraya ekleyeceÄŸiz)
from core.base import ModelAPIError
from core.gemini import GeminiModel 

# --- KONSTANTLAR ve YAPILANDIRMA ---
FILE_PATH_PATTERN = re.compile(r'\b[\w./-]+\.(py|js|html|css|md|json|txt|java|cpp|h|ts|jsx|tsx|sh)\b', re.IGNORECASE)
MAX_FILE_SIZE = 5_242_880  # 5MB
MAX_TOTAL_SIZE = 20_971_520 # 20MB
BACKUP_DIR = ".gassist_backups"
HISTORY_LOG = ".gassist_history.log"
MAX_BACKUPS_PER_FILE = 10 

# Renkli Terminal Ã‡Ä±ktÄ±sÄ±
class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    RESET = '\033[0m'
    
DRY_RUN = False
VERBOSE = False

# KullanÄ±labilir Model SÄ±nÄ±flarÄ± SÃ¶zlÃ¼ÄŸÃ¼
AVAILABLE_MODELS = {
    "1": GeminiModel,
}

# Hugging Face modelini gÃ¼venli bir ÅŸekilde yÃ¼kleme denemesi
try:
    from core.huggingface import HuggingFaceModel
    AVAILABLE_MODELS["2"] = HuggingFaceModel
except ImportError as e:
    print(f"{Colors.YELLOW}âš ï¸ UyarÄ±: Hugging Face modeli yÃ¼klenemedi. Detay: {e}{Colors.RESET}")
    print(f"{Colors.YELLOW}   LÃ¼tfen 'pip install requests' komutunu Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zdan emin olun.{Colors.RESET}")
except Exception as e:
    print(f"{Colors.YELLOW}âš ï¸ UyarÄ±: Hugging Face modÃ¼lÃ¼nde beklenmeyen hata: {e}{Colors.RESET}")

# --- GÃœVENLÄ°K ve UTILITY FONKSÄ°YONLARI ---
# (is_safe_path, backup_if_exists, log_command fonksiyonlarÄ± aynÄ± kalÄ±r)

def is_safe_path(file_path, current_directory):
    if os.path.isabs(file_path): return False
    normalized_path = os.path.normpath(file_path)
    if normalized_path.startswith('..'): return False
    full_path = os.path.join(current_directory, file_path)
    real_path = os.path.realpath(full_path) 
    if not real_path.startswith(current_directory): return False
    return True

def backup_if_exists(full_path):
    if os.path.exists(full_path) and os.path.isfile(full_path):
        os.makedirs(BACKUP_DIR, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_name = f"{os.path.basename(full_path)}.{timestamp}.backup"
        backup_path = os.path.join(BACKUP_DIR, backup_name)
        try:
            shutil.copy(full_path, backup_path)
        except Exception as e:
            print(f"{Colors.YELLOW}âš ï¸ Yedekleme HatasÄ±: {os.path.basename(full_path)} yedeklenemedi. Detay: {e}{Colors.RESET}")
            return None
        pattern = os.path.join(BACKUP_DIR, f"{os.path.basename(full_path)}.*.backup")
        backups = sorted(glob.glob(pattern))
        if len(backups) > MAX_BACKUPS_PER_FILE:
            for old_backup in backups[:len(backups) - MAX_BACKUPS_PER_FILE]:
                os.remove(old_backup)
                if VERBOSE:
                     print(f"{Colors.YELLOW}   ğŸ—‘ï¸ Eski yedek silindi: {os.path.basename(old_backup)}{Colors.RESET}")
        return backup_path
    return None

def log_command(prompt, files_saved_names):
    with open(HISTORY_LOG, 'a', encoding='utf-8') as f:
        f.write(f"\n{'='*60}\n")
        f.write(f"Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Komut: {prompt[:100]}...\n")
        f.write(f"SonuÃ§: {len(files_saved_names)} dosya oluÅŸturuldu/gÃ¼ncellendi: {', '.join(files_saved_names)}\n")

# --- YENÄ°: MODEL SEÃ‡Ä°MÄ° ---
def get_model_choice():
    """KullanÄ±cÄ±ya hangi modelin kullanÄ±lacaÄŸÄ±nÄ± sorar."""
    print(f"\n{Colors.BLUE}--- MEVCUT AI MODELLERÄ° ---{Colors.RESET}")
    for key, model_class in AVAILABLE_MODELS.items():
        print(f"  [{key}] {model_class.MODEL_NAME}")
    
    while True:
        choice = input(f"{Colors.YELLOW}KullanÄ±lacak modeli seÃ§in (Ã–rn: 1):{Colors.RESET} ").strip()
        if choice in AVAILABLE_MODELS:
            try:
                # SeÃ§ilen modelin istemcisini baÅŸlat
                model_instance = AVAILABLE_MODELS[choice]()
                print(f"{Colors.GREEN}âœ¨ Model seÃ§ildi: {model_instance.MODEL_NAME}{Colors.RESET}")
                return model_instance
            except ModelAPIError as e:
                print(f"{Colors.RED}Model HatasÄ±: {e}{Colors.RESET}")
                print(f"{Colors.YELLOW}LÃ¼tfen API anahtarÄ±nÄ±zÄ± veya ayarlarÄ±nÄ±zÄ± kontrol edin.{Colors.RESET}")
                continue
        else:
            print(f"{Colors.YELLOW}GeÃ§ersiz seÃ§im. LÃ¼tfen listeden bir sayÄ± girin.{Colors.RESET}")

# --- ANA FONKSÄ°YON ---
def get_assistant_response_and_save(prompt_text, model_instance):
    current_directory = os.getcwd()
    files_to_read = []
    
    # ... (1. ve 2. AdÄ±mlar: Dosya Okuma ve Prompt HazÄ±rlama aynÄ± kalÄ±r)
    potential_files = FILE_PATH_PATTERN.findall(prompt_text)
    
    for file_match in potential_files:
        file_path = file_match[0]
        full_path = os.path.join(current_directory, file_path)
        if os.path.exists(full_path) and os.path.isfile(full_path):
            try:
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                files_to_read.append(f"----- {file_path} -----\n{content}\n")
            except Exception as e:
                print(f"{Colors.YELLOW}UyarÄ±: '{file_path}' dosyasÄ± okunamadÄ±. ({e}){Colors.RESET}")

    context = "\n".join(files_to_read)
    if context:
        prompt_text = f"AÅŸaÄŸÄ±daki mevcut dosya iÃ§eriÄŸini ve yapÄ±sÄ±nÄ± dikkate alarak gÃ¶revi tamamla:\n\n{context}\n\n--- YENÄ° GÃ–REV ---\n{prompt_text}"
        
    system_instruction = (
        "Sen geliÅŸmiÅŸ bir Proje YÃ¶neticisi Yapay ZekasÄ±n. "
        "GÃ¶revin, istenen dosya yapÄ±sÄ±nÄ± (oluÅŸturma/gÃ¼ncelleme) saÄŸlamaktÄ±r. "
        "YanÄ±tÄ±n SADECE, dosya yollarÄ±nÄ± (klasÃ¶r dahil) anahtar, dosya iÃ§eriÄŸini ise deÄŸer olarak iÃ§eren tek bir JSON sÃ¶zlÃ¼ÄŸÃ¼ olmalÄ±dÄ±r. "
        "Dosya yollarÄ± gÃ¶receli olmalÄ±dÄ±r (Ã¶rn: 'src/config.py')."
        
        "\nÃ‡OK Ã–NEMLÄ°: JSON iÃ§eriÄŸinde (deÄŸerlerde), JSON ayrÄ±ÅŸtÄ±rÄ±cÄ±sÄ±nÄ± bozan Ã¶zel karakterler veya kaÃ§Ä±ÅŸ dizileri kullanma. TÃ¼m metin UTF-8 uyumlu olmalÄ±dÄ±r. TÃ¼m Ã§Ä±ktÄ±yÄ± tek bir ```json ... ``` bloÄŸunda ver."
        
        "\nÃ–RNEK JSON FORMATI: {'dosya/yolu.py': 'kod iÃ§eriÄŸi', 'README.md': 'metin iÃ§eriÄŸi'}"
    )
    
    print(f"{Colors.BLUE}âœ… GÃ–REV ALINDI:{Colors.RESET} {prompt_text.splitlines()[-1][:70]}...")

    try:
        # API Ã§aÄŸrÄ±sÄ±, seÃ§ilen model Ã¶rneÄŸi Ã¼zerinden yapÄ±lÄ±r
        full_response_text = model_instance.generate_content(
            system_instruction=system_instruction,
            prompt_text=prompt_text
        )
        
        # 3. JSON Ã‡Ä±ktÄ±sÄ±nÄ± GÃ¼venli Åekilde AyÄ±kla ve AyrÄ±ÅŸtÄ±r
        json_match = re.search(r"```json\n(.*?)```", full_response_text, re.DOTALL)
        
        if json_match:
            json_string = json_match.group(1).strip()
            if VERBOSE: print(f"{Colors.YELLOW}DEBUG: JSON Markdown bloÄŸu baÅŸarÄ±yla ayrÄ±ÅŸtÄ±rÄ±ldÄ±.{Colors.RESET}")
        else:
            print(f"{Colors.YELLOW}âš ï¸ UyarÄ±: YanÄ±tta beklenen JSON Markdown bloÄŸu bulunamadÄ±. Tam metinden ayrÄ±ÅŸtÄ±rma deneniyor...{Colors.RESET}")
            json_string = full_response_text.strip()
            
        # JSON yÃ¼klenirken hata yakalama
        try:
            json_string = json_string.replace('\u00ad', '').replace('\u200b', '').strip()
            file_map = json.loads(json_string)
            
        except json.JSONDecodeError as e:
            # ... (JSON Hata iÅŸleme aynÄ± kalÄ±r)
            print(f"{Colors.RED}--- JSON Ã‡Ã–ZÃœMLEME HATASI ---{Colors.RESET}")
            print(f"{Colors.RED}AI, geÃ§erli bir JSON formatÄ± dÃ¶ndÃ¼remedi. Detay: {e}{Colors.RESET}")
            print(f"{Colors.YELLOW}Ä°PUCU: Hata, genellikle README.md gibi Ã§ok satÄ±rlÄ± metinlerdeki hatalÄ± kaÃ§Ä±ÅŸ karakterlerinden kaynaklanÄ±r.{Colors.RESET}")
            raise e
            
        # ... (4, 5, 6, 7. AdÄ±mlar: Ã–n Kontrol, Onay, KayÄ±t ve Loglama aynÄ± kalÄ±r)
        files_to_save = {}
        total_size = 0
        
        if not isinstance(file_map, dict):
             raise ValueError(f"{Colors.RED}AI, sÃ¶zlÃ¼k formatÄ±nda (JSON Object) yanÄ±t vermedi.{Colors.RESET}")
             
        for file_path, content in file_map.items():
            content_str = str(content).strip()
            content_size = len(content_str.encode('utf-8'))
            
            if not is_safe_path(file_path, current_directory):
                print(f"{Colors.RED}ğŸš¨ GÃœVENLÄ°K UYARISI: ÅÃ¼pheli yol engellendi: {file_path}{Colors.RESET}")
                continue
            
            if content_size > MAX_FILE_SIZE:
                print(f"{Colors.YELLOW}âš ï¸ {file_path} Ã§ok bÃ¼yÃ¼k ({content_size/1024/1024:.2f}MB), atlanÄ±yor (Limit: {MAX_FILE_SIZE/1024/1024:.2f}MB).{Colors.RESET}")
                continue
                
            total_size += content_size
            if total_size > MAX_TOTAL_SIZE:
                print(f"{Colors.YELLOW}âš ï¸ Toplam dosya boyutu limitini aÅŸtÄ± ({total_size/1024/1024:.2f}MB). Kalan dosyalar atlanÄ±yor.{Colors.RESET}")
                break

            files_to_save[file_path] = content_str
        
        if not files_to_save:
             print("\nÄ°ÅŸlem yapÄ±lacak dosya bulunamadÄ±. Ä°ptal edildi.")
             return
             
        print("\nğŸ“‹ OLUÅTURULACAK/GÃœNCELLENECEK DOSYALAR:")
        for file_path, content in files_to_save.items():
             print(f"{Colors.BLUE}   - {file_path} ({len(content)} karakter, Boyut: {len(content.encode('utf-8'))/1024:.2f} KB){Colors.RESET}")
             
        if DRY_RUN:
             print(f"\n{Colors.YELLOW}ğŸ§ª [DRY-RUN MODU AKTÄ°F] Dosyalar kaydedilmeyecek, sadece gÃ¶sterildi.{Colors.RESET}")
             return
             
        confirm = input(f"\nDevam edilsin mi? (e/h): {Colors.YELLOW}").lower()
        print(Colors.RESET, end="") 
        if confirm != 'e':
             print(f"{Colors.YELLOW}Ä°ÅŸlem kullanÄ±cÄ± tarafÄ±ndan iptal edildi.{Colors.RESET}")
             return
             
        files_saved_names = []
        for file_path, content in files_to_save.items():
            full_path = os.path.join(current_directory, file_path)
            
            target_dir = os.path.dirname(full_path)
            if target_dir and not os.path.exists(target_dir):
                os.makedirs(target_dir, exist_ok=True)
            
            backup_path = backup_if_exists(full_path)
            if backup_path:
                print(f"{Colors.GREEN}   ğŸ“¦ Yedeklendi: {os.path.basename(backup_path)}{Colors.RESET}")

            with open(full_path, 'w', encoding='utf-8') as f:
                f.write(content)
                
            print(f"{Colors.GREEN}   -> KAYDEDÄ°LDÄ°/GÃœNCELLENDÄ°: {file_path}{Colors.RESET}")
            files_saved_names.append(file_path)
            
        print(f"\n{Colors.GREEN}âœ¨ BAÅARILI: Toplam {len(files_saved_names)} dosya oluÅŸturuldu/gÃ¼ncellendi.{Colors.RESET}")
        
        log_command(prompt_text, files_saved_names)


    # YENÄ° HATA YAKALAMA BLOKLARI (ModÃ¼l Ã¼zerinden gelen hatalarÄ± yakalar)
    except ModelAPIError as e:
        print(f"\n{Colors.RED}--- KRÄ°TÄ°K API HATASI ---{Colors.RESET}")
        print(f"{Colors.RED}API Ä°letiÅŸim HatasÄ±: {e}{Colors.RESET}")
        
    except Exception as e:
        if 'full_response_text' in locals() and full_response_text:
             print(f"\n{Colors.YELLOW}--- AI YANITI (HATA AYIKLAMA Ä°Ã‡Ä°N) ---{Colors.RESET}")
             print(full_response_text)
             print("------------------------------------------")
        else:
             print(f"\n{Colors.RED}--- KRÄ°TÄ°K HATA ---{Colors.RESET}")

        print(f"{Colors.RED}âŒ BEKLENMEYEN HATA: Proje kaydÄ± baÅŸarÄ±sÄ±z oldu. Detay: {e}{Colors.RESET}")

# --- ANA Ã‡ALIÅMA BLOÄU ---
if __name__ == "__main__":
    
    if "--dry-run" in sys.argv:
        DRY_RUN = True
        sys.argv.remove("--dry-run")
    if "--verbose" in sys.argv:
        VERBOSE = True
        sys.argv.remove("--verbose")

    if len(sys.argv) < 2:
        print(f"{Colors.YELLOW}KullanÄ±m:{Colors.RESET} gassist \"[GÃ¶reviniz Buraya]\" [--dry-run] [--verbose]")
        print(f"{Colors.YELLOW}Ã–rnek:{Colors.RESET} gassist \"src/app.js ve index.html oluÅŸtur.\" --dry-run")
        sys.exit(1)
    
    gorev_prompt = " ".join(sys.argv[1:]) 
    
    # Yeni: Modeli SeÃ§
    selected_model = get_model_choice()
    
    get_assistant_response_and_save(gorev_prompt, selected_model)
```

#### ğŸ“„ Dosya: `core/base.py`

```py
# core/base.py: Ortak ArayÃ¼z ve Hata TanÄ±mlarÄ±

class ModelAPIError(Exception):
    """API baÄŸlantÄ±/kota hatalarÄ± iÃ§in genel hata sÄ±nÄ±fÄ±."""
    pass

class BaseModel:
    """TÃ¼m model sÄ±nÄ±flarÄ±nÄ±n miras alacaÄŸÄ± soyut sÄ±nÄ±f."""
    MODEL_NAME = "Temel Model"

    def __init__(self):
        # API anahtarÄ±nÄ± kontrol etme vb.
        pass

    def generate_content(self, system_instruction, prompt_text):
        """AI'dan iÃ§erik Ã¼retme Ã§aÄŸrÄ±sÄ±."""
        raise NotImplementedError("Bu metot alt sÄ±nÄ±flar tarafÄ±ndan uygulanmalÄ±dÄ±r.")
```

#### ğŸ“„ Dosya: `core/gemini.py`

```py
# core/gemini.py: Google Gemini API UygulamasÄ±
import os
from google import genai
from google.genai import types
from google.genai.errors import APIError
from .base import BaseModel, ModelAPIError

class GeminiModel(BaseModel):
    MODEL_NAME = "Google Gemini (gemini-2.5-flash)"

    def __init__(self):
        try:
            self.client = genai.Client()
        except Exception as e:
            raise ModelAPIError(f"Gemini istemcisi baÅŸlatÄ±lamadÄ±: {e}")

    def generate_content(self, system_instruction, prompt_text):
        try:
            response = self.client.models.generate_content(
                model="gemini-2.5-flash",
                contents=[prompt_text],
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                )
            )
            return response.text.strip()
            
        except APIError as e:
            # API hatalarÄ±nÄ± genel ModelAPIError olarak yÃ¼kseltme
            error_message = getattr(e, 'message', str(e))
            raise ModelAPIError(f"Gemini API HatasÄ±: {error_message}")
```

#### ğŸ“„ Dosya: `core/huggingface.py`

```py
# core/huggingface.py: Hugging Face Inference API UygulamasÄ±
import os
import requests
import json
from .base import BaseModel, ModelAPIError

# Ã–rnek kodlama gÃ¶revleri iÃ§in gÃ¼Ã§lÃ¼ bir model
# Bu modelin API eriÅŸimi daha stabil olma eÄŸilimindedir.
# Code Llama'nÄ±n 7B Instruct versiyonunu deneyelim
# Yeni, daha stabil olduÄŸu varsayÄ±lan model:
# Yeni, kodlama odaklÄ± ve stabil olduÄŸu varsayÄ±lan model
DEFAULT_HF_MODEL = "meta-llama/Meta-Llama-3â€“8B-Instruct"
HF_API_URL_TEMPLATE = "https://router.huggingface.co/models/{model_id}"

class HuggingFaceModel(BaseModel):
    MODEL_NAME = f"Hugging Face ({DEFAULT_HF_MODEL})"

    def __init__(self):
        self.api_key = os.getenv("HUGGINGFACE_API_KEY")
        if not self.api_key:
            raise ModelAPIError("HUGGINGFACE_API_KEY ortam deÄŸiÅŸkeni ayarlanmadÄ±.")
        
        self.headers = {"Authorization": f"Bearer {self.api_key}"}
        self.api_url = HF_API_URL_TEMPLATE.format(model_id=DEFAULT_HF_MODEL)
        
    def generate_content(self, system_instruction, prompt_text):
        
        # Mistral formatÄ±nÄ± kullanarak system instruction ve prompt'u birleÅŸtirme
        full_prompt = (
            f"<s>[INST] <<SYS>>{system_instruction}<</SYS>>"
            f"GÃ¶revi tamamla ve SADECE JSON Ã§Ä±ktÄ±sÄ± ver: {prompt_text} [/INST]"
        )
        
        payload = {
            "inputs": full_prompt,
            "parameters": {
                "max_new_tokens": 2000,
                "temperature": 0.1,
                "return_full_text": False
            },
        }

        try:
            response = requests.post(self.api_url, headers=self.headers, json=payload)
            response.raise_for_status() # HTTP 4xx veya 5xx hatasÄ± varsa fÄ±rlatÄ±r

            response_json = response.json()
            
            # Hugging Face'in yanÄ±t formatÄ± genellikle bir liste dÃ¶ndÃ¼rÃ¼r.
            if not isinstance(response_json, list) or 'generated_text' not in response_json[0]:
                raise ModelAPIError(f"Hugging Face'ten beklenmedik yanÄ±t formatÄ± alÄ±ndÄ±: {response_json}")
                
            return response_json[0]['generated_text'].strip()

        except requests.exceptions.RequestException as e:
            # TÃ¼m requests hatalarÄ±nÄ± (baÄŸlantÄ±, timeout, HTTP hatalarÄ±) yakala
            raise ModelAPIError(f"Hugging Face API Ã§aÄŸrÄ±sÄ± baÅŸarÄ±sÄ±z oldu: {e}")
```

#### ğŸ“„ Dosya: `src/app.py`

```py
from flask import Flask, jsonify

app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello, World!'

@app.route('/users')
def get_users():
    users = [
        {'id': 1, 'name': 'Alice'},
        {'id': 2, 'name': 'Bob'},
        {'id': 3, 'name': 'Charlie'}
    ]
    return jsonify(users)

if __name__ == '__main__':
    app.run(debug=True)
```

#### ğŸ“„ Dosya: `src/handlers/user.py`

```py
Content preserved from original handlers/user.py
```
