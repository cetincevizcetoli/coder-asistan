# ğŸ“ Proje DÃ¶kÃ¼mÃ¼: coder-asistan

Bu dÃ¶kÃ¼m, **/home/ahmetc/proje/coder-asistan** dizini iÃ§in oluÅŸturulmuÅŸtur.
Not: `my_projects` klasÃ¶rÃ¼nÃ¼n iÃ§eriÄŸi gizlilik gereÄŸi hariÃ§ tutulmuÅŸtur.

### ğŸ“‚ Proje Dizin YapÄ±sÄ± ve Dosyalar

- **coder-asistan/** (Proje KÃ¶kÃ¼)
  - .gitignore
  - ARCHITECTURE.md
  - assistant.py
  - check_models.py
  - config.py
  - debug.py
  - generate_docs.py
  - launcher.py
  - migrate_projects.py
  - model_selector.py
  - proje_dokumu.md
  - readme.md
  - requirements.txt
  - settings_menu.py
  - system_audit.py
  - user_settings.json
  - **temp_install_dir/**
  - **core/**
    - base.py
    - deepseek.py
    - gemini.py
    - groq.py
    - huggingface.py
    - memory.py
    - orchestrator.py
  - **my_projects/** (KullanÄ±cÄ± Projeleri - Ä°Ã§erik Gizli)

---
### ğŸ’» Kod Ä°Ã§eriÄŸi DÃ¶kÃ¼mÃ¼


#### ğŸ“„ Dosya: `ARCHITECTURE.md`

```md
# ğŸ—ï¸ Coder-Asistan: Teknik Mimari ve GeliÅŸtirici KÄ±lavuzu

Bu belge, **Coder-Asistan** projesinin iÃ§ yapÄ±sÄ±nÄ±, veri akÄ±ÅŸÄ±nÄ±, tasarÄ±m kararlarÄ±nÄ± ve sistemin "neden" bÃ¶yle Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± anlatan teknik referanstÄ±r.

Proje, basit bir script deÄŸil; modÃ¼ler, RAG (Retrieval-Augmented Generation) tabanlÄ± ve durum (state) korumalÄ± bir **CLI Kodlama StÃ¼dyosu**dur.

---

## 1. ğŸ—ºï¸ KuÅŸ BakÄ±ÅŸÄ± Sistem Mimarisi

Sistem 4 ana katmandan oluÅŸur:

1.  **YÃ¶netim KatmanÄ± (Launcher):** KullanÄ±cÄ±yÄ± karÅŸÄ±lar, proje izolasyonunu saÄŸlar ve Ã§alÄ±ÅŸma dizinini ayarlar.
2.  **Beyin KatmanÄ± (Assistant & Config):** KullanÄ±cÄ± isteÄŸini iÅŸler, maliyeti hesaplar ve AI'ya "JSON formatÄ±nda" emir verir.
3.  **HafÄ±za KatmanÄ± (RAG Core):** Projedeki kodlarÄ± vektÃ¶rleÅŸtirir (Embedding) ve anlamsal arama yapar.
4.  **AdaptÃ¶r KatmanÄ± (Model Core):** FarklÄ± AI saÄŸlayÄ±cÄ±larÄ±nÄ± (Gemini, Groq, HF) tek bir standart arayÃ¼ze dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.


```text
      [ KULLANICI ]
           â”‚
           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   LAUNCHER.PY   â”‚  (1. GiriÅŸ KapÄ±sÄ± & Proje SeÃ§imi)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  ASSISTANT.PY   â”‚ â—„â”€â”€â”€â–º â”‚  CONFIG (Kurallar)â”‚
  â”‚ (Karar Motoru)  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€â–º [ ğŸ§  HAFIZA (RAG) ] â—„â”€â”€â”€ (.coder_memory)
           â”‚      (KodlarÄ± HatÄ±rlar)
           â”‚
           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   MODEL CORE    â”‚  (AdaptÃ¶r KatmanÄ±)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€â–º Google Gemini
           â”œâ”€â”€â”€â–º Groq Llama 3
           â””â”€â”€â”€â–º DeepSeek / HF

---

## 2. ğŸ“‚ Dizin YapÄ±sÄ± ve Sorumluluklar

```text
coder-asistan/
â”œâ”€â”€ launcher.py           # [ENTRY POINT] Proje seÃ§imi ve ortam hazÄ±rlÄ±ÄŸÄ±
â”œâ”€â”€ assistant.py          # [MAIN LOOP] Ä°stek-Cevap dÃ¶ngÃ¼sÃ¼ ve dosya iÅŸlemleri
â”œâ”€â”€ config.py             # [SETTINGS] Sabitler, Promptlar ve FiyatlandÄ±rma
â”œâ”€â”€ requirements.txt      # BaÄŸÄ±mlÄ±lÄ±klar
â”‚
â”œâ”€â”€ my_projects/          # [USER DATA] KullanÄ±cÄ± projelerinin fiziksel konumu
â”‚   â””â”€â”€ proje_x/          # -> Ä°zole Ã‡alÄ±ÅŸma AlanÄ±
â”‚       â”œâ”€â”€ .coder_memory/ # -> ChromaDB (VektÃ¶r VeritabanÄ±)
â”‚       â”œâ”€â”€ .chat_history/ # -> Loglar
â”‚       â””â”€â”€ src/           # -> KullanÄ±cÄ± KodlarÄ±
â”‚
â”œâ”€â”€ core/                 # [BACKEND] Sistem Ã§ekirdeÄŸi
â”‚   â”œâ”€â”€ base.py           # -> Soyut Model SÄ±nÄ±fÄ± (Interface)
â”‚   â”œâ”€â”€ memory.py         # -> RAG Motoru (SentenceTransformers + ChromaDB)
â”‚   â”œâ”€â”€ gemini.py         # -> Google Adapter
â”‚   â””â”€â”€ groq.py           # -> Groq Adapter
â”‚
â””â”€â”€ utils/ (Opsiyonel)    # YardÄ±mcÄ± araÃ§lar (debug.py, system_audit.py vb.)
```

---

## 3. âš™ï¸ Veri AkÄ±ÅŸÄ± (Bir Komutun YolculuÄŸu)

KullanÄ±cÄ± `python launcher.py` Ã§alÄ±ÅŸtÄ±rÄ±p bir projeye girdiÄŸinde ve "HatayÄ± dÃ¼zelt" dediÄŸinde arka planda ÅŸu olaylar zinciri gerÃ§ekleÅŸir:

### AdÄ±m 1: BaÄŸlamÄ±n YÃ¼klenmesi (Context Loading)
* `assistant.py`, `core.memory.MemoryManager`'Ä± baÅŸlatÄ±r.
* KullanÄ±cÄ±nÄ±n sorusu ("HatayÄ± dÃ¼zelt"), `SentenceTransformer` modeli ile **vektÃ¶re** (sayÄ±sal diziye) Ã§evrilir.
* ChromaDB iÃ§inde bu vektÃ¶re matematiksel olarak en yakÄ±n olan kod parÃ§alarÄ± (Chunks) bulunur.

### AdÄ±m 2: Prompt MÃ¼hendisliÄŸi (Prompt Engineering)
AI'ya giden metin ÅŸu ÅŸablonda birleÅŸtirilir:
1.  **Sistem Emri (`config.SYSTEM_INSTRUCTION`):** "Sen bir JSON makinesisin. Asla sohbet etme."
2.  **RAG BaÄŸlamÄ±:** "VeritabanÄ±ndan bulduÄŸum ilgili kodlar ÅŸunlar: ..."
3.  **KullanÄ±cÄ± Ä°steÄŸi:** "HatayÄ± dÃ¼zelt."

### AdÄ±m 3: Model Ã‡aÄŸrÄ±sÄ± ve Adaptasyon
* SeÃ§ili model (Ã–rn: Gemini), `core/gemini.py` Ã¼zerinden Ã§aÄŸrÄ±lÄ±r.
* Her model farklÄ± yanÄ±t verse de (Object, Dict, Text), adaptÃ¶rler bunu standart bir formata Ã§evirir.

### AdÄ±m 4: JSON TemizliÄŸi ve GÃ¼venlik
* AI'dan gelen yanÄ±t `clean_json_string()` fonksiyonuna girer. Markdown etiketleri (` ```json `) temizlenir.
* Saf JSON parse edilir.
* **GÃ¼venlik:** AI "BilgisayarÄ± kapat" diyemez. Sadece `dosya_olustur` veya `dosya_sil` komutlarÄ± iÅŸlenir.

### AdÄ±m 5: Ä°ÅŸlem ve Yedekleme
* Dosya yazÄ±lmadan Ã¶nce `backup_file()` fonksiyonu devreye girer.
* Hedef dosyanÄ±n bir kopyasÄ± `.gassist_backups` klasÃ¶rÃ¼ne zaman damgasÄ±yla (timestamp) kaydedilir.
* Yeni iÃ§erik yazÄ±lÄ±r.

---

## 4. ğŸ”§ Kritik KonfigÃ¼rasyonlar (`config.py`)

GeliÅŸtiricilerin bilmesi gereken hassas ayarlar:

* **`SYSTEM_INSTRUCTION`:** Sistemsel prompt. AI'nÄ±n "Suskun" olmasÄ±nÄ± saÄŸlayan yer burasÄ±dÄ±r. Buradaki kurallar gevÅŸetilirse sistemin JSON parse yeteneÄŸi bozulabilir.
* **`MAX_FILE_SIZE`:** VarsayÄ±lan 5MB. AI'nÄ±n token limitini patlatmamasÄ± iÃ§in bÃ¼yÃ¼k dosyalar (loglar, binaryler) okunmaz.
* **`PRICING_RATES`:** Maliyet hesaplama tablosu. Statiktir, API fiyatlarÄ± deÄŸiÅŸirse manuel gÃ¼ncellenmelidir.

---

## 5. ğŸ› ï¸ GeliÅŸtirici AraÃ§ Seti (DevTools)

Projeyi debug etmek veya yÃ¶netmek iÃ§in kullanÄ±lan "Ä°sviÃ§re Ã‡akÄ±sÄ±" araÃ§larÄ±:

| AraÃ§ | Dosya | GÃ¶revi |
| :--- | :--- | :--- |
| **HafÄ±za MÃ¼fettiÅŸi** | `debug.py` | ChromaDB veritabanÄ±na baÄŸlanÄ±r, vektÃ¶rleri ve kayÄ±tlÄ± kod parÃ§alarÄ±nÄ± ham haliyle gÃ¶sterir. |
| **Sistem Doktoru** | `system_audit.py` | Dosya izinlerini, log boyutlarÄ±nÄ± ve veritabanÄ± bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ (integrity) kontrol eder. |
| **Proje TaÅŸÄ±yÄ±cÄ±** | `migrate_projects.py` | `my_projects` dÄ±ÅŸÄ±ndaki "sahipsiz" projeleri bulup iÃ§eriye taÅŸÄ±r. |
| **Belgeleyici** | `generate_docs.py` | TÃ¼m kod yapÄ±sÄ±nÄ± tek bir Markdown dosyasÄ±na dÃ¶ker (LLM analizi iÃ§in). |

---

## 6. ğŸš€ Gelecek PlanlarÄ± ve GeniÅŸletilebilirlik

Bu mimari ÅŸunlara izin verecek ÅŸekilde tasarlanmÄ±ÅŸtÄ±r:
* **Yeni Model Ekleme:** Sadece `core/` altÄ±na yeni bir `.py` dosyasÄ± ekleyerek.
* **Resim DesteÄŸi:** `assistant.py` gÃ¼ncellenerek Gemini 1.5 Pro'nun Vision Ã¶zellikleri aÃ§Ä±labilir.
* **Web ArayÃ¼zÃ¼:** Logic (MantÄ±k) ve UI (ArayÃ¼z) ayrÄ±ldÄ±ÄŸÄ± iÃ§in, `launcher.py` yerine bir `app.py` (Flask/Streamlit) yazÄ±larak kolayca web'e taÅŸÄ±nabilir.

---
**GeliÅŸtirici:** Ahmet Ã‡etin
*Bu dokÃ¼man Coder-Asistan v2.0 mimarisini yansÄ±tÄ±r.*
```

#### ğŸ“„ Dosya: `assistant.py`

```py
import sys
import os
import re
import json
import shutil
import difflib
import time
from datetime import datetime
from typing import List, Optional, Any, Tuple, Dict

# --- PROJE MODÃœLLERÄ° ---
try:
    import config
    from config import Colors, PRICING_RATES
    from core.memory import MemoryManager
    from core.gemini import GeminiModel 
    from core.orchestrator import AgentOrchestrator
except ImportError:
    print("âš ï¸  Kritik modÃ¼ller yÃ¼klenemedi. LÃ¼tfen kÃ¼tÃ¼phanelerin yÃ¼klÃ¼ olduÄŸundan emin olun.")

# --- SABÄ°TLER ---
FILE_PATTERN = re.compile(r"[\w-]+\.(py|js|html|css|md|json|txt|java|cpp|h|ts|jsx|tsx|sh|env|sql|xml|yaml)", re.IGNORECASE)

# ==========================================
# ğŸ› ï¸ YARDIMCI FONKSÄ°YONLAR
# ==========================================

def is_safe_path(file_path: str, current_directory: str) -> bool:
    try:
        if os.path.isabs(file_path): return False
        if '..' in file_path: return False
        target_path = os.path.abspath(os.path.join(current_directory, file_path))
        safe_root = os.path.abspath(current_directory)
        return target_path.startswith(safe_root)
    except: return False

def show_diff(file_path: str, old_content: str, new_content: str):
    """Dosyadaki deÄŸiÅŸiklikleri terminalde gÃ¶rsel olarak gÃ¶sterir."""
    print(f"\n{Colors.BOLD}{Colors.CYAN}ğŸ” DEÄÄ°ÅÄ°KLÄ°K Ã–ZETÄ° ({file_path}):{Colors.RESET}")
    
    old_lines = old_content.splitlines(keepends=True)
    new_lines = new_content.splitlines(keepends=True)
    
    diff = difflib.unified_diff(old_lines, new_lines, fromfile='Eski', tofile='Yeni', n=0)
    
    has_changes = False
    for line in diff:
        has_changes = True
        if line.startswith('+') and not line.startswith('+++'):
            print(f"{Colors.GREEN}{line.strip()}{Colors.RESET}")
        elif line.startswith('-') and not line.startswith('---'):
            print(f"{Colors.RED}{line.strip()}{Colors.RESET}")
        elif line.startswith('@@'):
            print(f"{Colors.MAGENTA}{line.strip()}{Colors.RESET}")
            
    if not has_changes:
        print(f"{Colors.GREY}DeÄŸiÅŸiklik tespit edilmedi (iÃ§erik aynÄ±).{Colors.RESET}")    

def clean_json_string(json_string: str) -> Optional[Dict]:
    if isinstance(json_string, dict): return json_string
    try:
        if "```" in json_string:
            lines = json_string.split('\n')
            clean_lines = []
            capture = False
            for line in lines:
                if "```" in line:
                    capture = not capture
                    continue
                if capture:
                    clean_lines.append(line)
            json_string = "\n".join(clean_lines) if clean_lines else json_string.replace("```json", "").replace("```", "")

        json_string = json_string.strip()
        if json_string.rfind('}') != -1:
            json_string = json_string[:json_string.rfind('}')+1]
        
        return json.loads(json_string)
    except Exception:
        return None

def backup_file(full_path: str) -> Optional[str]:
    if not os.path.exists(full_path): return None
    os.makedirs(config.BACKUP_DIR, exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_name = f"{os.path.basename(full_path)}.{timestamp}.backup"
    shutil.copy(full_path, os.path.join(config.BACKUP_DIR, backup_name))
    return backup_name

def log_conversation(working_dir: str, user_prompt: str, ai_explanation: str, model_name: str, cost: float = 0.0):
    log_file = os.path.join(working_dir, ".chat_history.log")
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M')
    log_entry = (
        f"{'â•'*60}\nğŸ“… ZAMAN: {timestamp} | ğŸ¤– MODEL: {model_name}\n"
        f"ğŸ’° MALÄ°YET: ${cost:.5f}\nğŸ‘¤ USER: {user_prompt}\nğŸ¤– AI:   {ai_explanation}\n"
    )
    try:
        with open(log_file, "a", encoding="utf-8") as f: f.write(log_entry)
    except: pass

def update_project_stats(working_dir: str, usage_data: dict, model_key: str) -> Tuple[float, float]:
    stats_file = os.path.join(working_dir, ".project_stats.json")
    stats = {"total_cost": 0.0, "total_input_tokens": 0, "total_output_tokens": 0, "last_updated": ""}
    if os.path.exists(stats_file):
        try:
            with open(stats_file, 'r', encoding='utf-8') as f: stats = json.load(f)
        except: pass

    in_tokens = usage_data.get("input_tokens", 0)
    out_tokens = usage_data.get("output_tokens", 0)
    rates = PRICING_RATES.get(model_key, {"input": 0, "output": 0})
    current_cost = ((in_tokens / 1_000_000) * rates["input"]) + ((out_tokens / 1_000_000) * rates["output"])

    stats["total_cost"] += current_cost
    stats["total_input_tokens"] += in_tokens
    stats["total_output_tokens"] += out_tokens
    stats["last_updated"] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    try:
        with open(stats_file, 'w', encoding='utf-8') as f: json.dump(stats, f, indent=4)
    except: pass
    return current_cost, stats["total_cost"]

# ==========================================
# ğŸš€ ANA Ä°ÅLEM MOTORU (AGENTIC)
# ==========================================

def process_single_turn(prompt_text: str, orchestrator: AgentOrchestrator, working_dir: str, memory: Any, is_dry_run: bool = False):
    """Hibrit RAG baÄŸlamÄ± ile isteÄŸi akÄ±llÄ±ca yÃ¶nlendirir."""
    
    # 1. HÄ°BRÄ°T HAFIZA SORGUSU (BaÄŸlamÄ± her durumda alÄ±yoruz)
    rag_context = ""
    if memory:
        print(f"{Colors.CYAN}ğŸ” Hibrit HafÄ±za taranÄ±yor...{Colors.RESET}")
        rag_context = memory.query(prompt_text, n_results=config.MAX_CONTEXT_RESULTS)
        if len(rag_context) > config.MAX_CONTEXT_CHARS:
            rag_context = rag_context[:config.MAX_CONTEXT_CHARS] + "\n...(KÄ±rpÄ±ldÄ±)..."

    # --- ğŸ›¡ï¸ 1. GÃœNCELLENMÄ°Å SÄ°STEMSEL KOMUT FÄ°LTRESÄ° (KISA DEVRE) ---
    sorgu_temiz = prompt_text.lower().strip()
    sistem_komutlari = ["tara", "indeksle", "hafÄ±zayÄ± gÃ¼ncelle", "yenile", "reindex"]
    aksiyon_kelimeleri = ["yerine", "yap", "deÄŸiÅŸtir", "ekle", "dÃ¼zelt", "sil"]
    
    # Sadece sistemle ilgiliyse ve deÄŸiÅŸiklik/aksiyon iÃ§ermiyorsa kÄ±sa devre yap
    if any(k in sorgu_temiz for k in sistem_komutlari) and not any(x in sorgu_temiz for x in aksiyon_kelimeleri):
        print(f"{Colors.GREEN}âš™ï¸  Sistem iÅŸlemi algÄ±landÄ±, dosyalar taranÄ±yor...{Colors.RESET}")
        if memory:
            files = [f for f in os.listdir(working_dir) if FILE_PATTERN.match(f)]
            memory.index_files(files)
            print(f"{Colors.GREEN}âœ… HafÄ±za gÃ¼ncellendi. ArtÄ±k sorularÄ±nÄ±zÄ± sorabilirsiniz.{Colors.RESET}")
            return 

    # --- ğŸ›¡ï¸ 2. FÄ°LTRE: BÄ°LGÄ° SORGUSU / SORU MODU ---
    soru_kelimeleri = ["nedir", "kaÃ§", "nasÄ±l", "kim", "nerede", "neden", "bilgi ver", "anlat", "?"]
    is_question = any(q in sorgu_temiz for q in soru_kelimeleri)

    if is_question and not any(x in sorgu_temiz for x in aksiyon_kelimeleri):
        print(f"{Colors.MAGENTA}â„¹ï¸  Soru algÄ±landÄ±, doÄŸrudan yanÄ±tlanÄ±yor...{Colors.RESET}")
        try:
            raw_res = orchestrator.developer.generate_content(
                "Sen bilgili bir yazÄ±lÄ±m asistanÄ±sÄ±n. SADECE kullanÄ±cÄ±ya bilgi ver. Kod yazma, dosya deÄŸiÅŸtirme planÄ± yapma.",
                f"HAFIZADAN GELEN BÄ°LGÄ°LER:\n{rag_context}\n\nKULLANICI SORUSU: {prompt_text}"
            )
            content = raw_res["content"] if isinstance(raw_res, dict) else str(raw_res)
            print(f"\n{Colors.MAGENTA}ğŸ¤– CEVAP:{Colors.RESET} {Colors.CYAN}{content}{Colors.RESET}")
            
            if isinstance(raw_res, dict):
                current_cost, total_cost = update_project_stats(working_dir, raw_res.get("usage", {}), raw_res.get("model_key", ""))
                print(f"{Colors.GREY}ğŸ“Š Maliyet: ${current_cost:.5f}{Colors.RESET}")
            return
        except Exception as e:
            print(f"{Colors.RED}Soru yanÄ±tlanÄ±rken hata oluÅŸtu: {e}{Colors.RESET}")
            return

    # --- 3. NORMAL AKIÅ: ORCHESTRATOR (MÄ°MAR + MÃœHENDÄ°S) ---
    raw_response = orchestrator.execute_workflow(prompt_text, rag_context, working_dir)
    
    if not raw_response:
        print(f"{Colors.YELLOW}âš ï¸ Ä°ÅŸlem durduruldu veya iptal edildi.{Colors.RESET}")
        return

    # 4. YANIT ANALÄ°ZÄ°
    if isinstance(raw_response, dict):
        content = raw_response.get("content", "")
        usage = raw_response.get("usage", {})
        model_key = raw_response.get("model_key", "unknown")
    else:
        content = raw_response
        usage = {}
        model_key = "unknown"

    ai_response_plan = clean_json_string(content)
    
    if ai_response_plan is None:
        print(f"{Colors.RED}âŒ MÃ¼hendis yanÄ±tÄ± JSON formatÄ±nda deÄŸil. Ä°ÅŸlem iptal edildi.{Colors.RESET}")
        return

    # Ä°statistik ve Maliyet
    current_cost, total_cost = update_project_stats(working_dir, usage, model_key)
    print(f"\n{Colors.GREY}ğŸ“Š Ä°ÅŸlem Maliyeti: {Colors.GREEN}${current_cost:.5f}{Colors.RESET} (Proje ToplamÄ±: ${total_cost:.5f})")

    # 5. UYGULAMA (Dosya Ä°ÅŸlemleri)
    explanation = ai_response_plan.get("aciklama", "Ä°ÅŸlem tamamlandÄ±.")
    files_create = ai_response_plan.get("dosya_olustur", {})
    files_delete = ai_response_plan.get("dosya_sil", [])

    print(f"\n{Colors.MAGENTA}ğŸ¤– SONUÃ‡:{Colors.RESET} {Colors.CYAN}{explanation}{Colors.RESET}")
    
    if is_dry_run: return

    # Silme
    for p in files_delete:
        if is_safe_path(p, working_dir):
            full = os.path.join(working_dir, p)
            if os.path.exists(full):
                backup_file(full); os.remove(full)
                print(f"{Colors.RED}ğŸ—‘ï¸ Silindi: {p}{Colors.RESET}")

    # Yazma
    new_files = []
    # ... (Dosya Yazma DÃ¶ngÃ¼sÃ¼ Ä°Ã§inde) ...
    for p, content in files_create.items():
        if is_safe_path(p, working_dir):
            full = os.path.join(working_dir, p)
            try:
                os.makedirs(os.path.dirname(full), exist_ok=True)
                
                # Ã–zet GÃ¶sterimi:
                old_text = ""
                if os.path.exists(full):
                    with open(full, 'r', encoding='utf-8') as f:
                        old_text = f.read()
                
                # DeÄŸiÅŸiklikleri ekrana bas
                show_diff(p, old_text, content)
                
                # KayÄ±t Ä°ÅŸlemi
                if os.path.exists(full): backup_file(full)
                with open(full, 'w', encoding='utf-8') as f: f.write(content)
                print(f"{Colors.GREEN}ğŸ’¾ YazÄ±ldÄ± ve HafÄ±zaya AlÄ±ndÄ±: {p}{Colors.RESET}")
                new_files.append(p)
            except Exception as e:
                 print(f"{Colors.RED}Dosya hatasÄ± ({p}): {e}{Colors.RESET}")

    # HafÄ±zayÄ± GÃ¼ncelle (Yeni yazÄ±lan dosyalarÄ± otomatik indeksle)
    if memory and new_files:
        memory.index_files(new_files)

    log_conversation(working_dir, prompt_text, explanation, "Agent-Workflow", current_cost)

# ==========================================
# ğŸŒŸ PROJE ANA DÃ–NGÃœSÃœ
# ==========================================

def main(project_name):
    project_path = os.path.abspath(os.path.join(config.PROJECTS_DIR, project_name))
    
    if not os.path.exists(project_path):
        print(f"{Colors.RED}Hata: {project_path} yolu mevcut deÄŸil.{Colors.RESET}")
        return

    print(f"\n{Colors.GREEN}ğŸš€ OTURUM BAÅLATILDI: {project_name.upper()}{Colors.RESET}")

    try:
        orchestrator = AgentOrchestrator()
        memory = MemoryManager(project_root=project_path)
    except Exception as e:
        print(f"{Colors.RED}BaÅŸlatma hatasÄ±: {e}{Colors.RESET}")
        return

    print(f"{Colors.CYAN}{'â”'*50}{Colors.RESET}")
    print(f"Sohbet Aktif. 'q': Ã§Ä±kÄ±ÅŸ | 'b': ana menÃ¼")
    
    while True:
        try:
            user_input = input(f"{Colors.BOLD}{Colors.YELLOW}({project_name}) > {Colors.RESET}").strip()
            
            if user_input.lower() in ['exit', 'q', 'quit', 'b']:
                break
            
            if not user_input: continue
            
            process_single_turn(user_input, orchestrator, project_path, memory)
            
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"{Colors.RED}Beklenmedik Hata: {e}{Colors.RESET}")

if __name__ == "__main__":
    print("LÃ¼tfen 'launcher.py' kullanÄ±n.")
```

#### ğŸ“„ Dosya: `check_models.py`

```py
import os
import sys

# google-genai yÃ¼klÃ¼ mÃ¼ kontrol et
try:
    from google import genai
except ImportError:
    print("âŒ HATA: 'google-genai' kÃ¼tÃ¼phanesi bulunamadÄ±.")
    sys.exit(1)

api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    print("âŒ HATA: GOOGLE_API_KEY tanÄ±mlÄ± deÄŸil!")
    sys.exit(1)

print(f"ğŸ”‘ Anahtar ile baÄŸlanÄ±lÄ±yor... (Son 4 hane: {api_key[-4:]})")

try:
    client = genai.Client(api_key=api_key)
    print("\nğŸ“¡ --- HESABINIZDA AKTÄ°F OLAN MODELLER ---")
    
    count = 0
    # Modelleri Ã§ek ve listele
    # Pager Ã¼zerinden dÃ¶ner, listeye Ã§evirelim
    for m in client.models.list():
        # Sadece iÃ§erik Ã¼retebilen modelleri al
        if "generateContent" in m.supported_actions:
            # Ä°smi temizle (models/ Ã¶nekini at)
            clean_name = m.name.replace('models/', '')
            print(f"âœ… {clean_name}")
            count += 1
            
    if count == 0:
        print("\nâš ï¸ HATA: HiÃ§bir model bulunamadÄ±. API Key'inizin yetkilerini kontrol edin.")
    else:
        print("\nğŸ‘‰ Ä°PUCU: YukarÄ±daki âœ… ile baÅŸlayan isimlerden birini config.py dosyasÄ±na kopyalayÄ±n.")

except Exception as e:
    print(f"\nâŒ BAÄLANTI HATASI: {e}")
```

#### ğŸ“„ Dosya: `config.py`

```py
import os

# ==========================================
# ğŸ¨ RENK AYARLARI
# ==========================================
class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    MAGENTA = '\033[95m'
    GREY = '\033[90m'
    BOLD = '\033[1m'
    RESET = '\033[0m'

# ==========================================
# âš™ï¸ SÄ°STEM VE DOSYA AYARLARI
# ==========================================
MAX_FILE_SIZE = 5 * 1024 * 1024
MAX_TOTAL_SIZE = 20 * 1024 * 1024
BACKUP_DIR = ".gassist_backups"
MAX_BACKUPS_PER_FILE = 5
MEMORY_DIR_NAME = ".coder_memory"
COLLECTION_NAME = "project_codebase"
EMBEDDING_MODEL = "paraphrase-multilingual-MiniLM-L12-v2"
MAX_CONTEXT_RESULTS = 3
MAX_CONTEXT_CHARS = 12000
MAX_BACKUPS_PER_FILE = 10


# YENÄ°: Projelerin toplanacaÄŸÄ± ana klasÃ¶r
PROJECTS_DIR = "my_projects"

# ==========================================
# ğŸ’° MALÄ°YET VE KATMAN
# ==========================================
USER_TIER = 'free' 
PRICING_RATES = {
    "gemini-2.5-flash-lite": { "input": 0.075, "output": 0.30 },
    "gemini-2.5-flash": { "input": 0.10, "output": 0.40 },
    "llama-3.3-70b-versatile": { "input": 0.59, "output": 0.79 },
    "deepseek-chat": { "input": 0.14, "output": 0.28 },
    "Qwen/Qwen2.5-Coder-7B-Instruct": { "input": 0.0, "output": 0.0 }
}

# ==========================================
# ğŸ¤– MODEL AYARLARI
# ==========================================

MODEL_CONFIGS = {
    "gemini": {
        "env_var": "GOOGLE_API_KEY",
        "model_name": "gemini-2.5-flash-lite", 
        "display_name": "Google Gemini 2.5 Flash Lite",
    },
    "groq": {
        "env_var": "GROQ_API_KEY",
        "model_id": "llama-3.3-70b-versatile",
        "display_name": "Groq Llama 3.3 70B",
    },
    "deepseek": {
        "env_var": "DEEPSEEK_API_KEY",
        "model_id": "deepseek-chat",
        "display_name": "DeepSeek Chat",
    },
    "huggingface": {
        "env_var": "HUGGINGFACE_API_KEY",
        "model_id": "Qwen/Qwen2.5-Coder-7B-Instruct",
        "display_name": "Hugging Face Qwen",
    }
}
ACTIVE_PROFILE = 'gemini'
# ==========================================
# ğŸš€ AKTÄ°F PROFÄ°L SEÃ‡Ä°MÄ° (Eksik Olan KÄ±sÄ±m)
# ==========================================
# Buraya MODEL_CONFIGS iÃ§indeki anahtarlardan birini yazmalÄ±sÄ±n:
# SeÃ§enekler: 'gemini', 'groq', 'deepseek', 'huggingface'


# ==========================================
# ğŸ§  YENÄ° AI SÄ°STEM TALÄ°MATI (AkÄ±llÄ± JSON Modu)
# ==========================================
# config.py iÃ§indeki SYSTEM_INSTRUCTION kÄ±smÄ±nÄ± ÅŸununla deÄŸiÅŸtirin veya ekleyin:

# MÄ°MAR Ä°Ã‡Ä°N (Groq)
ARCHITECT_INSTRUCTION = (
    "Sen uzman bir yazÄ±lÄ±m mimarÄ±sÄ±n. GÃ¶revin, kullanÄ±cÄ± isteÄŸini analiz etmek ve bir uygulama planÄ± Ã§Ä±karmaktÄ±r.\n"
    "KURALLAR:\n"
    "1. Kod yazma, sadece hangi dosyalarÄ±n neden deÄŸiÅŸmesi gerektiÄŸini aÃ§Ä±kla.\n"
    "2. YanÄ±tÄ±n ÅŸu JSON formatÄ±nda olmalÄ±:\n"
    "{\n"
    "  'plan': 'AdÄ±m adÄ±m yapÄ±lacak iÅŸlemler listesi',\n"
    "  'etkilenecek_dosyalar': ['dosya1.py', 'dosya2.py']\n"
    "}"
)

# MÃœHENDÄ°S Ä°Ã‡Ä°N (Gemini)
DEVELOPER_INSTRUCTION = (
    "Sen uzman bir yazÄ±lÄ±m geliÅŸtiricisin. MimarÄ±n sunduÄŸu plana gÃ¶re kodlarÄ± yazmalÄ±sÄ±n.\n"
    "KURALLAR:\n"
    "1. Sadece geÃ§erli bir JSON objesi dÃ¶ndÃ¼r.\n"
    "2. Format:\n"
    "{\n"
    "  'aciklama': 'YapÄ±lan iÅŸlemin Ã¶zeti',\n"
    "  'dosya_olustur': {'yol': 'icerik'},\n"
    "  'dosya_sil': []\n"
    "}"
)

# ==========================================
# ğŸ§  HAFIZA PROFÄ°LLERÄ° (MenÃ¼de GÃ¶rÃ¼necekler)
# ==========================================
MEMORY_PROFILES = {
    "1": {
        "model_name": "all-MiniLM-L6-v2",
        "display": "Hafif (Light)",
        "desc": "ğŸš€ En HÄ±zlÄ±sÄ± | DÃ¼ÅŸÃ¼k RAM | 384 Boyut | Genel projeler iÃ§in ideal.",
        "dim": 384
    },
    "2": {
        "model_name": "paraphrase-multilingual-MiniLM-L12-v2",
        "display": "Dengeli (Medium)",
        "desc": "âš–ï¸  Daha Ä°yi TÃ¼rkÃ§e | Orta HÄ±z | 384 Boyut | KarmaÅŸÄ±k metinler iÃ§in.",
        "dim": 384
    },
    "3": {
        "model_name": "all-mpnet-base-v2",
        "display": "GÃ¼Ã§lÃ¼ (Heavy)",
        "desc": "ğŸ§  En YÃ¼ksek DoÄŸruluk | YavaÅŸ | 768 Boyut | Akademik/Derin analiz iÃ§in.",
        "dim": 768
    }
}
# ==========================================
# ğŸš€ AKTÄ°F MODEL VE HAFIZA SEÃ‡Ä°MÄ°
# ==========================================
# SeÃ§enekler: 'gemini', 'groq', 'deepseek', 'huggingface'
ACTIVE_MODEL = "gemini" 

# HafÄ±za AyarÄ±
EMBEDDING_MODEL = "paraphrase-multilingual-MiniLM-L12-v2"

```

#### ğŸ“„ Dosya: `debug.py`

```py
import os
import sys
from pathlib import Path
from core.memory import MemoryManager
import config
from config import Colors

def inspect_project():
    workspace = Path.cwd() / config.PROJECTS_DIR
    projects = [d for d in workspace.iterdir() if d.is_dir() and (d / ".coder_memory").exists()]
    
    if not projects:
        print(f"{Colors.RED}Ä°ncelenecek aktif hafÄ±zalÄ± proje bulunamadÄ±.{Colors.RESET}")
        return

    print(f"\n{Colors.CYAN}ğŸ•µï¸ HAFIZA MÃœFETTÄ°ÅÄ°: Proje SeÃ§in{Colors.RESET}")
    for idx, p in enumerate(projects, 1):
        print(f"[{idx}] {p.name}")
    
    choice = input("\nSeÃ§im: ")
    if not choice.isdigit() or int(choice) > len(projects): return
    
    target_proj = projects[int(choice)-1]
    memory = MemoryManager(str(target_proj))
    
    while True:
        print(f"\n{Colors.YELLOW}--- {target_proj.name} HafÄ±za MenÃ¼sÃ¼ ---{Colors.RESET}")
        print("[1] Anlamsal Sorgu Testi (RAG Test)")
        print("[2] TÃ¼m KayÄ±tlÄ± DosyalarÄ± Listele")
        print("[3] Belirli Bir DosyanÄ±n HafÄ±zasÄ±nÄ± Sil")
        print("[Q] Ã‡Ä±kÄ±ÅŸ")
        
        sub_choice = input("\nSeÃ§im: ").lower()
        
        if sub_choice == '1':
            q = input("ğŸ” AI gibi bir soru sorun: ")
            res = memory.query(q)
            print(f"\n{Colors.GREEN}ğŸ” BULUNAN BAÄLAM:{Colors.RESET}\n{res}")
            
        elif sub_choice == '2':
            res = memory.collection.get()
            print(f"\n{Colors.CYAN}ğŸ“‘ Ä°NDEKSLENMÄ°Å DOSYALAR:{Colors.RESET}")
            for mid in res['ids']: print(f"  - {mid}")
            
        elif sub_choice == '3':
            fname = input("Silinecek dosya yolu (Ã¶rn: main.py): ")
            try:
                memory.collection.delete(ids=[fname])
                print(f"{Colors.RED}ğŸ—‘ï¸ {fname} hafÄ±zadan silindi.{Colors.RESET}")
            except: print("Hata: Dosya bulunamadÄ±.")
            
        elif sub_choice == 'q': break

if __name__ == "__main__":
    inspect_project()
```

#### ğŸ“„ Dosya: `generate_docs.py`

```py
import os
import sys

# ==========================================
# âš™ï¸ AYARLAR VE FÄ°LTRELER
# ==========================================

# Sadece iÃ§eriÄŸi taranmayacak sistem klasÃ¶rleri
DIKKATE_ALINMAYACAK_DIZINLER = [
    '.git', '__pycache__', 'venv', '.venv', 'env', '.env', 'node_modules', 
    '.vscode', '.idea', 'dist', 'build', 'target', 'bin',
    '__macosx', '.ds_store', 'logs', 'site-packages', 'lib', 'include',
    '.gassist_backups', '.coder_memory'
]

# Ä°Ã§eriÄŸi dÃ¶kÃ¼lmeyecek ama varlÄ±ÄŸÄ± gÃ¶sterilecek "Ã–zel" klasÃ¶rler
OZEL_USER_KLASORLERI = ['my_projects']

# Ä°Ã§eriÄŸi dÃ¶kÃ¼me eklenecek kod uzantÄ±larÄ±
BELGELENECEK_KOD_UZANTILARI = [
    '.py', '.php', '.js', '.html', '.css', '.json', '.xml', '.yaml', '.yml', 
    '.sh', '.bash', '.c', '.cpp', '.h', '.hpp', '.java', '.go', '.rb', '.swift', 
    '.kt', '.ts', '.jsx', '.tsx', '.conf', '.ini', '.sql', '.md', '.txt'
]

# Ã‡Ä±ktÄ± dosyasÄ±nÄ±n adÄ±
CIKTI_DOSYASI = "proje_dokumu.md"

# ==========================================
# ğŸ› ï¸ FONKSÄ°YONLAR
# ==========================================

def dosya_icerigini_getir(yol):
    """Dosya iÃ§eriÄŸini okur ve Markdown kod bloÄŸu iÃ§inde dÃ¶ndÃ¼rÃ¼r."""
    try:
        with open(yol, 'r', encoding='utf-8') as f:
            icerik = f.read()
            
        uzanti = os.path.splitext(yol)[1].lstrip('.').lower()
        return f"\n```{(uzanti if uzanti else 'plaintext')}\n{icerik}\n```\n"
    except Exception as e:
        return f"\n> [OkunamadÄ±: {e}]\n"

def dizin_yapisi_getir(hedef_dizin):
    """Verilen yoldan baÅŸlayarak dizin yapÄ±sÄ±nÄ± dÃ¶ndÃ¼rÃ¼r."""
    yapÄ± = "### ğŸ“‚ Proje Dizin YapÄ±sÄ± ve Dosyalar\n\n"
    
    for kok, dizinler, dosyalar in os.walk(hedef_dizin):
        # Filtreleme: Gereksiz klasÃ¶rleri gezme
        dizinler[:] = [d for d in dizinler if d.lower() not in DIKKATE_ALINMAYACAK_DIZINLER]
        
        yol_parcalari = kok.lower().split(os.sep)
        if any(yasak in yol_parcalari for yasak in DIKKATE_ALINMAYACAK_DIZINLER):
            continue

        base_name = os.path.basename(kok)
        goreli_yol = os.path.relpath(kok, hedef_dizin)
        
        # AÄŸaÃ§ yapÄ±sÄ± baÅŸlÄ±ÄŸÄ±
        if goreli_yol == '.':
            seviye = 0
            yapÄ± += f"- **{os.path.basename(hedef_dizin)}/** (Proje KÃ¶kÃ¼)\n"
        else:
            seviye = goreli_yol.count(os.sep) + 1
            girinti = "  " * seviye
            
            # Ã–zel klasÃ¶r kontrolÃ¼ (my_projects gibi)
            if base_name in OZEL_USER_KLASORLERI:
                yapÄ± += f"{girinti}- **{base_name}/** (KullanÄ±cÄ± Projeleri - Ä°Ã§erik Gizli)\n"
                dizinler[:] = [] # AltÄ±na inme
                continue 
            else:
                yapÄ± += f"{girinti}- **{base_name}/**\n"

        girinti_dosya = "  " * (seviye + 1)
        
        # DOSYALARI LÄ°STELEME (Filtresiz)
        for dosya in sorted(dosyalar):
            # .git klasÃ¶rÃ¼ iÃ§indeki dosyalarÄ± hariÃ§ tut, gerisi gelsin
            if '.git' in yol_parcalari: continue
            
            yapÄ± += f"{girinti_dosya}- {dosya}\n"
                    
    return yapÄ±

def ana_fonksiyon():
    hedef_dizin = os.getcwd() 
    proje_adi = os.path.basename(hedef_dizin)
    
    dokum_metni = f"# ğŸ“ Proje DÃ¶kÃ¼mÃ¼: {proje_adi}\n\n"
    dokum_metni += f"Bu dÃ¶kÃ¼m, **{hedef_dizin}** dizini iÃ§in oluÅŸturulmuÅŸtur.\n"
    dokum_metni += "Not: `my_projects` klasÃ¶rÃ¼nÃ¼n iÃ§eriÄŸi gizlilik gereÄŸi hariÃ§ tutulmuÅŸtur.\n\n"
    
    print(f"1/3: '{proje_adi}' klasÃ¶r yapÄ±sÄ± taranÄ±yor...")
    dokum_metni += dizin_yapisi_getir(hedef_dizin)
    
    dokum_metni += "\n---\n"
    dokum_metni += "### ğŸ’» Kod Ä°Ã§eriÄŸi DÃ¶kÃ¼mÃ¼\n\n"
    
    print("2/3: Kod iÃ§erikleri toplanÄ±yor...")
    
    dosya_sayisi = 0
    for kok, dizinler, dosyalar in os.walk(hedef_dizin):
        dizinler[:] = [d for d in dizinler if d.lower() not in DIKKATE_ALINMAYACAK_DIZINLER]
        
        if os.path.basename(kok) in OZEL_USER_KLASORLERI:
            dizinler[:] = []
            continue

        yol_parcalari = kok.lower().split(os.sep)
        if any(yasak in yol_parcalari for yasak in DIKKATE_ALINMAYACAK_DIZINLER): continue

        for dosya in sorted(dosyalar):
            dosya_yolu = os.path.join(kok, dosya)
            
            # KENDÄ°SÄ°NÄ° VE Ã‡IKTI DOSYASINI OKUMASIN (Ä°Ã§erik DÃ¶kÃ¼mÃ¼nde)
            if dosya == CIKTI_DOSYASI: continue
            
            uzanti = os.path.splitext(dosya)[1].lower()

            if uzanti in BELGELENECEK_KOD_UZANTILARI:
                goreli_yol = os.path.relpath(dosya_yolu, hedef_dizin)
                dokum_metni += f"\n#### ğŸ“„ Dosya: `{goreli_yol}`\n"
                dokum_metni += dosya_icerigini_getir(dosya_yolu)
                dosya_sayisi += 1
            
    print(f"3/3: '{CIKTI_DOSYASI}' dosyasÄ±na kayÄ±t yapÄ±lÄ±yor...")
    try:
        cikti_yolu = os.path.join(hedef_dizin, CIKTI_DOSYASI)
        with open(cikti_yolu, 'w', encoding='utf-8') as f:
            f.write(dokum_metni)
        print(f"\nâœ… Ä°ÅŸlem BaÅŸarÄ±lÄ±! Toplam {dosya_sayisi} dosya belgelendi.")
    except Exception as e:
        print(f"\nâŒ Hata: {e}")
        
if __name__ == "__main__":
    ana_fonksiyon()
```

#### ğŸ“„ Dosya: `launcher.py`

```py
import os
import sys
import json
import time
import datetime
import importlib

try:
    import config
except ImportError:
    print("HATA: config.py bulunamadÄ±!")
    sys.exit(1)

# ==========================================
# AYAR SEÃ‡ENEKLERÄ°
# ==========================================
MEMORY_OPTIONS = {
    "1": {"id": "all-MiniLM-L6-v2", "name": "Hafif (Light)", "desc": "ğŸš€ HÄ±zlÄ±"},
    "2": {"id": "paraphrase-multilingual-MiniLM-L12-v2", "name": "Dengeli (Medium)", "desc": "âš–ï¸ TÃ¼rkÃ§e"},
    "3": {"id": "all-mpnet-base-v2", "name": "GÃ¼Ã§lÃ¼ (Heavy)", "desc": "ğŸ§  DetaylÄ±"}
}

MODEL_OPTIONS = {
    "1": {"id": "gemini", "name": "Google Gemini", "desc": "âš¡ Dengeli ve Ãœcretsiz"},
    "2": {"id": "groq", "name": "Groq (Llama 3)", "desc": "ğŸš€ IÅŸÄ±k HÄ±zÄ±nda"},
    "3": {"id": "deepseek", "name": "DeepSeek Chat", "desc": "ğŸ‘¨â€ğŸ’» Kodlama UzmanÄ±"},
    "4": {"id": "huggingface", "name": "Hugging Face", "desc": "ğŸ¤— AÃ§Ä±k Kaynak Modeller"}
}

def clear_screen():
    os.system('cls' if os.name == 'nt' else 'clear')

def update_config_file(variable_name, new_value):
    try:
        with open("config.py", "r", encoding="utf-8") as f: lines = f.readlines()
        with open("config.py", "w", encoding="utf-8") as f:
            found = False
            for line in lines:
                if line.strip().startswith(variable_name):
                    f.write(f'{variable_name} = "{new_value}"\n'); found = True
                else: f.write(line)
            if not found: f.write(f'\n{variable_name} = "{new_value}"\n')
        return True
    except: return False

def load_projects():
    projects = []
    if not os.path.exists(config.PROJECTS_DIR):
        try: os.makedirs(config.PROJECTS_DIR)
        except: pass
    try: items = os.listdir(config.PROJECTS_DIR)
    except: items = []
    for item in items:
        path = os.path.join(config.PROJECTS_DIR, item)
        if os.path.isdir(path):
            meta_path = os.path.join(path, "metadata.json")
            embedding_model = "BÄ°LÄ°NMÄ°YOR"
            cost = 0.0
            last_date = "0"
            if os.path.exists(meta_path):
                try:
                    with open(meta_path, 'r') as f:
                        data = json.load(f)
                        embedding_model = data.get("embedding_model", "Eski")
                        cost = data.get("total_cost", 0.0)
                        last_date = data.get("last_interaction", "0")
                except: pass
            projects.append({"name": item, "embedding_model": embedding_model, "cost": cost, "last_date": last_date})
    return sorted(projects, key=lambda x: x['last_date'], reverse=True)

def create_new_project():
    print(f"\n{config.Colors.CYAN}âœ¨ Yeni Proje{config.Colors.RESET}")
    name = input("Proje Ä°smi: ").strip()
    if not name: return
    path = os.path.join(config.PROJECTS_DIR, name)
    if os.path.exists(path): print("Zaten var!"); time.sleep(1); return
    try:
        os.makedirs(path)
        metadata = {
            "created_at": str(datetime.datetime.now()),
            "embedding_model": config.EMBEDDING_MODEL,
            "total_cost": 0.0,
            "last_interaction": str(datetime.datetime.now())
        }
        with open(os.path.join(path, "metadata.json"), 'w') as f: json.dump(metadata, f, indent=4)
        start_project(name, config.EMBEDDING_MODEL)
    except Exception as e: print(f"Hata: {e}"); input("Enter...")

def start_project(name, project_embed_model):
    if project_embed_model != config.EMBEDDING_MODEL:
        print(f"\n{config.Colors.RED}â›” UYUMSUZLUK: Bu proje '{project_embed_model}' kullanÄ±yor.{config.Colors.RESET}")
        print("Ayarlardan hafÄ±za modelini deÄŸiÅŸtirmeniz gerek."); input("Enter..."); return

    print(f"\n{config.Colors.GREEN}ğŸš€ Sistem BaÅŸlatÄ±lÄ±yor...{config.Colors.RESET}")
    try:
        # Kodun en baÅŸÄ±na import assistant eklemek yerine burada dene
        import assistant
        importlib.reload(assistant)
        assistant.main(name) 
    except Exception as e:
        # Hata olduÄŸunda ekranÄ±n temizlenmesini engellemek iÃ§in:
        print(f"\n{config.Colors.RED}âŒ KRÄ°TÄ°K HATA OLUÅTU:{config.Colors.RESET}")
        import traceback
        traceback.print_exc() # HatanÄ±n tam yerini ve nedenini yazar
        input(f"\n{config.Colors.YELLOW}Devam etmek iÃ§in Enter'a basÄ±n (Hata kaybolmadan okuyun)...{config.Colors.RESET}")

def settings_menu():
    while True:
        clear_screen()
        print(f"{config.Colors.YELLOW}=== AYARLAR ==={config.Colors.RESET}")
        print(f"1. Yapay Zeka Modeli : {config.Colors.GREEN}{config.ACTIVE_MODEL.upper()}{config.Colors.RESET}")
        print(f"2. HafÄ±za (RAG) Tipi : {config.Colors.CYAN}{config.EMBEDDING_MODEL}{config.Colors.RESET}")
        print("-" * 50)
        print("[M] Model DeÄŸiÅŸtir")
        print("[H] HafÄ±za DeÄŸiÅŸtir")
        print("[X] Geri DÃ¶n")
        
        sel = input("\nSeÃ§im: ").strip().upper()
        
        if sel == 'X': break
        
        elif sel == 'M':
            print(f"\n{config.Colors.BLUE}--- MODEL SEÃ‡Ä°MÄ° ---{config.Colors.RESET}")
            for k, v in MODEL_OPTIONS.items():
                mark = " (AKTÄ°F)" if v['id'] == config.ACTIVE_MODEL else ""
                print(f"[{k}] {v['name']}{mark} - {v['desc']}")
            m_sel = input("SeÃ§im: ").strip()
            if m_sel in MODEL_OPTIONS:
                new_val = MODEL_OPTIONS[m_sel]['id']
                update_config_file("ACTIVE_MODEL", new_val)
                print("â™»ï¸  Kaydedildi, yeniden baÅŸlatÄ±lÄ±yor..."); time.sleep(1)
                os.execv(sys.executable, ['python'] + sys.argv)

        elif sel == 'H':
            print(f"\n{config.Colors.BLUE}--- HAFIZA SEÃ‡Ä°MÄ° ---{config.Colors.RESET}")
            for k, v in MEMORY_OPTIONS.items():
                mark = " (AKTÄ°F)" if v['id'] == config.EMBEDDING_MODEL else ""
                print(f"[{k}] {v['name']}{mark} - {v['desc']}")
            h_sel = input("SeÃ§im: ").strip()
            if h_sel in MEMORY_OPTIONS:
                new_val = MEMORY_OPTIONS[h_sel]['id']
                update_config_file("EMBEDDING_MODEL", new_val)
                print("â™»ï¸  Kaydedildi, yeniden baÅŸlatÄ±lÄ±yor..."); time.sleep(1)
                os.execv(sys.executable, ['python'] + sys.argv)

def main():
    while True:
        clear_screen()
        importlib.reload(config)
        projects = load_projects()
        
        print(f"{config.Colors.BOLD}{config.Colors.BLUE}=== AI ASÄ°STAN (v2.4) ==={config.Colors.RESET}")
        print(f"ğŸ¤– Model : {config.Colors.GREEN}{config.ACTIVE_MODEL.upper()}{config.Colors.RESET}")
        print(f"ğŸ§  HafÄ±za: {config.Colors.YELLOW}{config.EMBEDDING_MODEL}{config.Colors.RESET}")
        print("-" * 60)
        
        for idx, p in enumerate(projects, 1):
            status = "âœ…" if p['embedding_model'] == config.EMBEDDING_MODEL else "â›”"
            print(f"[{idx}] {p['name']:<15} {status} ({p['embedding_model']})")
            
        print("-" * 60)
        print("[N] Yeni Proje  |  [S] Ayarlar  |  [Q] Ã‡Ä±kÄ±ÅŸ")
        ch = input("> ").strip().upper()
        
        if ch == 'Q': sys.exit()
        elif ch == 'N': create_new_project()
        elif ch == 'S': settings_menu()
        elif ch.isdigit():
            idx = int(ch) - 1
            if 0 <= idx < len(projects): start_project(projects[idx]['name'], projects[idx]['embedding_model'])

if __name__ == "__main__":
    main()
```

#### ğŸ“„ Dosya: `migrate_projects.py`

```py
import os
import shutil
from pathlib import Path

# Hedef
TARGET_DIR = Path("my_projects")
if not TARGET_DIR.exists():
    os.makedirs(TARGET_DIR)

print("ğŸšš Proje TaÅŸÄ±ma Ä°ÅŸlemi BaÅŸlÄ±yor...")

# Mevcut dizindeki klasÃ¶rleri tara
for entry in Path.cwd().iterdir():
    # Kendi dizinimizdeki klasÃ¶rler (my_projects hariÃ§)
    if entry.is_dir() and entry.name != "my_projects" and entry.name != "core" and entry.name != "venv" and not entry.name.startswith("."):
        
        # EÄŸer iÃ§inde .coder_memory varsa bu bir projedir!
        if (entry / ".coder_memory").exists():
            print(f"ğŸ“¦ Bulundu ve TaÅŸÄ±nÄ±yor: {entry.name}")
            try:
                shutil.move(str(entry), str(TARGET_DIR / entry.name))
                print(f"   âœ… TaÅŸÄ±ndÄ±.")
            except Exception as e:
                print(f"   âŒ Hata: {e}")

print("\nğŸ Ä°ÅŸlem Tamam. ArtÄ±k launcher.py'yi Ã§alÄ±ÅŸtÄ±rabilirsiniz.")
```

#### ğŸ“„ Dosya: `model_selector.py`

```py
# model_selector.py
import os
from config import Colors, MODEL_CONFIGS

def check_api_key(env_var):
    """Ortam deÄŸiÅŸkeninde API anahtarÄ± var mÄ± kontrol eder."""
    key = os.getenv(env_var)
    return key is not None and len(key) > 0

def get_available_models():
    """Sistemdeki kullanÄ±labilir modelleri dinamik olarak tarar."""
    available = {}
    
    # 1. Gemini KontrolÃ¼
    gemini_conf = MODEL_CONFIGS["gemini"]
    if check_api_key(gemini_conf["env_var"]):
        try:
            from core.gemini import GeminiModel
            available["1"] = {
                "class": GeminiModel,
                "name": gemini_conf["display_name"],
                "status": f"{Colors.GREEN}âœ… HazÄ±r{Colors.RESET}"
            }
        except ImportError:
            available["1"] = {"status": f"{Colors.RED}âŒ KÃ¼tÃ¼phane eksik (google-genai){Colors.RESET}"}
    else:
        available["1"] = {
            "name": gemini_conf["display_name"],
            "status": f"{Colors.RED}âŒ API Key Eksik ({gemini_conf['env_var']}){Colors.RESET}"
        }

    # 2. Hugging Face KontrolÃ¼
    hf_conf = MODEL_CONFIGS["huggingface"]
    if check_api_key(hf_conf["env_var"]):
        try:
            from core.huggingface import HuggingFaceModel
            available["2"] = {
                "class": HuggingFaceModel,
                "name": hf_conf["display_name"],
                "status": f"{Colors.GREEN}âœ… HazÄ±r{Colors.RESET}"
            }
        except ImportError:
             available["2"] = {"status": f"{Colors.RED}âŒ KÃ¼tÃ¼phane eksik (requests){Colors.RESET}"}
    else:
        available["2"] = {
            "name": hf_conf["display_name"],
            "status": f"{Colors.RED}âŒ API Key Eksik ({hf_conf['env_var']}){Colors.RESET}"
        }

    return available

def select_model_interactive():
    """KullanÄ±cÄ±ya interaktif seÃ§im menÃ¼sÃ¼ sunar."""
    available = get_available_models()
    
    print(f"\n{Colors.BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print(f"â•‘       ğŸ¤–  AI MODEL SEÃ‡Ä°M EKRANI        â•‘")
    print(f"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Colors.RESET}\n")

    ready_models = {}
    
    for key, info in available.items():
        # EÄŸer 'class' anahtarÄ± varsa model Ã§alÄ±ÅŸtÄ±rÄ±labilir demektir
        if "class" in info:
            ready_models[key] = info["class"]
            print(f"  [{key}] {info['name']}  {info['status']}")
        else:
            print(f"  [{key}] {info.get('name', 'Bilinmeyen')}  {info['status']}")

    if not ready_models:
        print(f"\n{Colors.RED}âš ï¸  HÄ°Ã‡BÄ°R MODEL KULLANILABÄ°LÄ°R DURUMDA DEÄÄ°L!{Colors.RESET}")
        print(f"{Colors.YELLOW}LÃ¼tfen .bashrc dosyasÄ±na API anahtarlarÄ±nÄ±zÄ± ekleyin.{Colors.RESET}")
        return None

    # VarsayÄ±lan olarak ilk hazÄ±r modeli seÃ§
    default_key = list(ready_models.keys())[0]
    
    print(f"\n{Colors.CYAN}VarsayÄ±lan Model: {available[default_key]['name']} (Enter'a bas){Colors.RESET}")
    choice = input(f"{Colors.YELLOW}SeÃ§iminiz [1/2]: {Colors.RESET}").strip()
    
    if not choice:
        choice = default_key
        
    if choice in ready_models:
        try:
            return ready_models[choice]()
        except Exception as e:
            print(f"{Colors.RED}Model baÅŸlatÄ±lÄ±rken hata oluÅŸtu: {e}{Colors.RESET}")
            return None
    else:
        print(f"{Colors.RED}GeÃ§ersiz seÃ§im.{Colors.RESET}")
        return None
```

#### ğŸ“„ Dosya: `readme.md`

```md
# ğŸš€ Coder-Asistan
### HafÄ±zalÄ±, GÃ¼venli ve Proje OdaklÄ± AI Kodlama StÃ¼dyosu

![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Status](https://img.shields.io/badge/status-active-success)

**Coder-Asistan**, klasik "soru sor â€“ cevap al" botlarÄ±ndan farklÄ± olarak, projelerinizi yÃ¶neten, baÄŸlamÄ± hatÄ±rlayan ve kodu **kontrollÃ¼ ÅŸekilde** deÄŸiÅŸtiren **terminal tabanlÄ± bir AI geliÅŸtirme ortamÄ±dÄ±r.**

Her proje iÃ§in ayrÄ± bir hafÄ±za oluÅŸturur (RAG). Bir projede Ã¶ÄŸrendiÄŸini diÄŸerine taÅŸÄ±maz. Ne yaptÄ±ÄŸÄ±nÄ± Ã¶nce planlar, sonra siz onaylarsanÄ±z uygular.

> **KÄ±saca:** Bu bir bot deÄŸil, **AI destekli bir geliÅŸtirme Ã§alÄ±ÅŸma alanÄ±**.

---

## ğŸ¯ Temel Ã–zellikler

* **ğŸ­ Proje FabrikasÄ± (`launcher.py`):** TÃ¼m projeleri tek merkezden yÃ¶netin.
* **ğŸ§  Ä°zole HafÄ±za:** Her projenin kendi `.coder_memory` klasÃ¶rÃ¼ vardÄ±r. AI, projenizdeki dosyalarÄ± okur ve hatÄ±rlar.
* **ğŸ›¡ï¸ GÃ¼venli Mod:** KodlarÄ± doÄŸrudan yazmaz; Ã¶nce plan sunar, onaylarsanÄ±z iÅŸler.
* **ğŸ’° Maliyet Takibi:** Token baÅŸÄ±na harcamayÄ± kuruÅŸu kuruÅŸuna raporlar.
* **ğŸ”Œ Model Ã–zgÃ¼rlÃ¼ÄŸÃ¼:** Google Gemini, Llama 3 (Groq), DeepSeek veya Hugging Face.

---

## ğŸ“¦ Kurulum (2 Dakikada HazÄ±r)

### 1ï¸âƒ£ Projeyi Ä°ndirin
```bash
git clone [https://github.com/cetincevizcetoli/coder-asistan.git](https://github.com/cetincevizcetoli/coder-asistan.git)
cd coder-asistan
```

### 2ï¸âƒ£ Sanal Ortam OluÅŸturun (Ã–NEMLÄ°)

**ğŸªŸ Windows:**
```cmd
python -m venv venv
venv\Scripts\activate
```

**ğŸ§ Linux / macOS:**
```bash
python3 -m venv venv
source venv/bin/activate
```
*(Terminal satÄ±rÄ±nÄ±n baÅŸÄ±nda `(venv)` yazÄ±sÄ±nÄ± gÃ¶rmelisiniz.)*

### 3ï¸âƒ£ Paketleri YÃ¼kleyin
```bash
pip install -r requirements.txt
```

---

## ğŸ”‘ API AnahtarÄ± AyarlarÄ±

Coder-Asistan bir beyne ihtiyaÃ§ duyar. **Google Gemini (Ãœcretsiz)** Ã¶nerilir.

1. [Google AI Studio](https://aistudio.google.com/app/apikey) adresinden Key alÄ±n.
2. BilgisayarÄ±nÄ±za kaydedin:

**ğŸªŸ Windows (KalÄ±cÄ±):**
```cmd
setx GOOGLE_API_KEY "API_KEY_BURAYA_YAPISTIR"
```
*(Komuttan sonra terminali kapatÄ±p yeniden aÃ§Ä±n!)*

**ğŸ§ Linux / macOS:**
```bash
echo 'export GOOGLE_API_KEY="API_KEY_BURAYA"' >> ~/.bashrc
source ~/.bashrc
```

---

## â–¶ï¸ NasÄ±l KullanÄ±lÄ±r? (Ana Kumanda)

TÃ¼m sistemi yÃ¶netmek iÃ§in tek bir komut yeterlidir:

```bash
python launcher.py
```

KarÅŸÄ±nÄ±za gelen menÃ¼den:
* **[N]** ile yeni proje oluÅŸturabilir,
* **[1-9]** ile mevcut projelerinize girip AI ile Ã§alÄ±ÅŸmaya baÅŸlayabilirsiniz.
* **[E]** ile projelerinizi ZIP olarak yedekleyebilirsiniz.

---

## ğŸ› ï¸ Ä°sviÃ§re Ã‡akÄ±sÄ±: YardÄ±mcÄ± AraÃ§lar

Bu projede sadece kod yazan bir asistan yok, iÅŸinizi kolaylaÅŸtÄ±racak bir dizi **profesyonel araÃ§** bulunur. Ä°ÅŸte alet Ã§antanÄ±z:

### 1. ğŸ•µï¸â€â™‚ï¸ HafÄ±za MÃ¼fettiÅŸi (`debug.py`)
AI'nÄ±n projeniz hakkÄ±nda ne bildiÄŸini merak mÄ± ediyorsunuz? VektÃ¶r veritabanÄ±nÄ±n iÃ§ine girip kaydedilen kod parÃ§alarÄ±nÄ± okumanÄ±zÄ± saÄŸlar.
```bash
python debug.py
```
* **Ne zaman kullanÄ±lÄ±r?** AI, kodunuzu hatÄ±rlamÄ±yorsa veya yanlÄ±ÅŸ cevap veriyorsa hafÄ±zayÄ± kontrol etmek iÃ§in.

### 2. ğŸšš Proje Nakliyecisi (`migrate_projects.py`)
Eski sÃ¼rÃ¼mden kalma veya yanlÄ±ÅŸlÄ±kla ana dizine kopyaladÄ±ÄŸÄ±nÄ±z projeleri bulur ve otomatik olarak yeni sisteme (`my_projects` klasÃ¶rÃ¼ne) taÅŸÄ±r.
```bash
python migrate_projects.py
```
* **Ne zaman kullanÄ±lÄ±r?** KlasÃ¶rde projeniz var ama Launcher listesinde gÃ¶rÃ¼nmÃ¼yorsa.

### 3. ğŸ©º Sistem Doktoru (`system_audit.py`)
Projelerinizin saÄŸlÄ±k kontrolÃ¼nÃ¼ yapar. Log dosyalarÄ± dolu mu? VeritabanÄ± bÃ¼tÃ¼nlÃ¼ÄŸÃ¼ (integrity) saÄŸlam mÄ±? Hepsini raporlar.
```bash
python system_audit.py
```
* **Ne zaman kullanÄ±lÄ±r?** Sistemsel bir hatadan ÅŸÃ¼pheleniyorsanÄ±z veya veritabanÄ± bozulduysa.

### 4. ğŸ“ Proje Katibi (`generate_docs.py`)
TÃ¼m projenizin kodlarÄ±nÄ± okur ve tek bir Markdown dosyasÄ±nda (`proje_dokumu.md`) birleÅŸtirir.
```bash
python generate_docs.py
```
* **Ne zaman kullanÄ±lÄ±r?** Projenin tamamÄ±nÄ± ChatGPT/Claude gibi baÅŸka bir AI'ya atÄ±p "Bunu analiz et" demek istediÄŸinizde.

### 5. ğŸ“¡ Model KontrolcÃ¼sÃ¼ (`check_models.py`)
Google hesabÄ±nÄ±zda tanÄ±mlÄ± ve eriÅŸilebilir olan Gemini modellerini listeler.
```bash
python check_models.py
```
* **Ne zaman kullanÄ±lÄ±r?** "Hangi modelleri kullanabilirim?" diye merak ettiÄŸinizde.

---

## ğŸ§ª GeliÅŸmiÅŸ Parametreler

Projeye girdikten sonra (veya `assistant.py`'yi manuel kullanÄ±rken) ÅŸu modlarÄ± kullanabilirsiniz:

* **`--dry-run` (Prova Modu):**
  AI kodlarÄ± yazar, planÄ± gÃ¶sterir ama **dosyaya kaydetmez.**
  ```bash
  python assistant.py "Ana sayfayÄ± deÄŸiÅŸtir" --dry-run
  ```

* **`--verbose` (Geveze Mod):**
  Arka planda dÃ¶nen ham JSON verisini ve dÃ¼ÅŸÃ¼nce sÃ¼recini gÃ¶sterir. Hata ayÄ±klamak iÃ§in idealdir.
  ```bash
  python assistant.py "Hata bul" --verbose
  ```

---

## ğŸ—ï¸ Proje Mimarisi

```text
coder-asistan/
â”œâ”€ launcher.py          # ğŸ® ANA KUMANDA (BaÅŸlatÄ±cÄ±)
â”œâ”€ assistant.py         # ğŸ§  Ä°ÅŸlem Motoru
â”œâ”€ my_projects/         # ğŸ“‚ PROJE FABRÄ°KASI
â”‚  â””â”€ odev-1/           # ğŸ”’ Ä°zole Proje AlanÄ±
â”‚     â”œâ”€ .coder_memory/ # ğŸ§  Projeye Ã¶zel hafÄ±za
â”‚     â””â”€ src/           # KodlarÄ±nÄ±z
â”œâ”€ debug.py             # ğŸ•µï¸â€â™‚ï¸ HafÄ±za MÃ¼fettiÅŸi
â”œâ”€ system_audit.py      # ğŸ©º Sistem Doktoru
â”œâ”€ migrate_projects.py  # ğŸšš TaÅŸÄ±yÄ±cÄ±
â””â”€ requirements.txt
```

---

## ğŸ’¡ Ä°puÃ§larÄ± ve PÃ¼f NoktalarÄ±

* **ğŸ—‘ï¸ Proje Silme:** Bir projeyi silmek iÃ§in Launcher'da bir komut yoktur. `my_projects` klasÃ¶rÃ¼ne gidip ilgili proje klasÃ¶rÃ¼nÃ¼ **elle silmeniz** yeterlidir.
* **ğŸ§  HafÄ±za SÄ±fÄ±rlama:** AI eski kodlarÄ± hatÄ±rlamakta Ä±srar ediyorsa veya kafasÄ± karÄ±ÅŸtÄ±ysa; proje klasÃ¶rÃ¼nÃ¼zdeki `.coder_memory` klasÃ¶rÃ¼nÃ¼ silin. Coder-Asistan bir sonraki aÃ§Ä±lÄ±ÅŸta dosyalarÄ± tarayÄ±p hafÄ±zayÄ± sÄ±fÄ±rdan kuracaktÄ±r.
* **âš™ï¸ Ä°nce Ayarlar:** Dosya boyutu sÄ±nÄ±rlarÄ±nÄ± veya maliyet hesaplama yÃ¶ntemini deÄŸiÅŸtirmek isterseniz `config.py` dosyasÄ±nÄ± dÃ¼zenleyebilirsiniz.

---
## ğŸ¤ KatkÄ±da Bulunma

Pull request'ler kabul edilir! BÃ¼yÃ¼k deÄŸiÅŸiklikler iÃ§in Ã¶nce bir Issue aÃ§arak tartÄ±ÅŸalÄ±m.

> ğŸ—ï¸ **GeliÅŸtirici Notu:** Bu projenin iÃ§ yapÄ±sÄ±nÄ±, veri akÄ±ÅŸÄ±nÄ± ve teknik detaylarÄ±nÄ± derinlemesine incelemek iÃ§in lÃ¼tfen **[MÄ°MARÄ° VE TEKNÄ°K KILAVUZ (ARCHITECTURE.md)](ARCHITECTURE.md)** dosyasÄ±nÄ± okuyunuz.
---
## ğŸ‘¤ GeliÅŸtirici

**Ahmet Ã‡etin**
* **GitHub:** [github.com/cetincevizcetoli](https://github.com/cetincevizcetoli)
* **Web:** [yapanzeka.acetin.com.tr](https://yapanzeka.acetin.com.tr)

> *"KarmaÅŸÄ±k kodlarÄ±, kontrollÃ¼ araÃ§larla yÃ¶netin."*
```

#### ğŸ“„ Dosya: `requirements.txt`

```txt
google-genai
requests
openai
chromadb>=0.4.0
sentence-transformers>=2.2.0
torch>=2.0.0
rank_bm25  # <--- Hibrit (Keyword) arama motoru iÃ§in yeni eklendi
termcolor
tqdm

```

#### ğŸ“„ Dosya: `settings_menu.py`

```py
import os
import json
import time

# Renkler
GREEN = '\033[92m'
RED = '\033[91m'
YELLOW = '\033[93m'
CYAN = '\033[96m'
RESET = '\033[0m'

SETTINGS_FILE = "user_settings.json"

# HafÄ±za Profilleri
PROFILES = {
    'light':  {'name': 'Hafif (Light)',  'size': '80 MB',  'desc': 'ğŸš€ Ã‡ok HÄ±zlÄ±, DÃ¼ÅŸÃ¼k RAM (Z570 Ä°Ã§in Ã–nerilen)'},
    'medium': {'name': 'Orta (Medium)',  'size': '470 MB', 'desc': 'âš–ï¸ Daha Ä°yi TÃ¼rkÃ§e, Dengeli HÄ±z'},
    'heavy':  {'name': 'AÄŸÄ±r (Heavy)',   'size': '2.2 GB', 'desc': 'ğŸ‹ï¸ Ã‡ok YavaÅŸ, GPU Ä°ster (Dikkat!)'}
}

def load_settings():
    if os.path.exists(SETTINGS_FILE):
        try:
            with open(SETTINGS_FILE, 'r') as f:
                return json.load(f)
        except: pass
    return {"active_profile": "light"}

def save_settings(settings):
    try:
        with open(SETTINGS_FILE, 'w') as f:
            json.dump(settings, f, indent=4)
    except Exception as e:
        print(f"{RED}Ayarlar kaydedilemedi: {e}{RESET}")

def settings_main():
    while True:
        os.system('cls' if os.name == 'nt' else 'clear')
        current_settings = load_settings()
        curr_profile = current_settings.get("active_profile", "light")
        
        if curr_profile not in PROFILES:
            curr_profile = 'light'

        print(f"{CYAN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—{RESET}")
        print(f"{CYAN}â•‘           âš™ï¸  SÄ°STEM AYARLARI                 â•‘{RESET}")
        print(f"{CYAN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{RESET}")
        
        p_info = PROFILES[curr_profile]
        
        print(f"\nğŸ§  Aktif HafÄ±za Modeli: {GREEN}{p_info['name']} [{p_info['size']}]{RESET}")
        print(f"{YELLOW}   â””â”€ {p_info['desc']}{RESET}\n")
        
        print("Mevcut SeÃ§enekler:")
        print(f"   [1] Hafif  (80 MB)  - HÄ±z OdaklÄ±")
        print(f"   [2] Orta   (470 MB) - Anlama OdaklÄ±")
        print(f"   [3] AÄŸÄ±r   (2 GB)   - Detay OdaklÄ±")
        print(f"\n   [Q] MenÃ¼ye DÃ¶n")
        
        choice = input(f"\n{YELLOW}SeÃ§iminiz: {RESET}").strip().lower()
        
        if choice == 'q':
            break
            
        new_profile = None
        if choice == '1': new_profile = 'light'
        elif choice == '2': new_profile = 'medium'
        elif choice == '3': new_profile = 'heavy'
        
        if new_profile and new_profile != curr_profile:
            print(f"\n{RED}âš ï¸  DÄ°KKAT: HafÄ±za Modeli DeÄŸiÅŸtiriliyor!{RESET}")
            print(f"Eski model ile oluÅŸturulan proje hafÄ±zalarÄ±, bu model ile Ã§alÄ±ÅŸmaz.")
            print(f"Sistem, projeye girdiÄŸinde eski hafÄ±zayÄ± otomatik olarak {RED}ARÅÄ°VLEYÄ°P SIFIRLAYACAKTIR.{RESET}")
            
            # --- DÃœZELTME BURADA ---
            # ArtÄ±k hem 'evet' hem 'e' kabul ediyor
            confirm = input(f"\n{RED}OnaylÄ±yor musunuz? (e/h): {RESET}").lower()
            
            if confirm in ['evet', 'e']:
                current_settings["active_profile"] = new_profile
                save_settings(current_settings)
                print(f"\n{GREEN}âœ… Ayar deÄŸiÅŸtirildi: {new_profile.upper()}{RESET}")
                time.sleep(1.5)
            else:
                print(f"\n{YELLOW}Ä°ptal edildi.{RESET}")
                time.sleep(1)
        elif new_profile == curr_profile:
            print(f"\n{YELLOW}Zaten bu mod aktif.{RESET}")
            time.sleep(1)

if __name__ == "__main__":
    settings_main()
```

#### ğŸ“„ Dosya: `system_audit.py`

```py
import os
import sys
import sqlite3
from pathlib import Path

# Config dosyasÄ±ndan proje klasÃ¶rÃ¼nÃ¼ Ã¶ÄŸrenelim
try:
    import config
    PROJECTS_DIR_NAME = config.PROJECTS_DIR
except ImportError:
    PROJECTS_DIR_NAME = "my_projects"

# Renkler
GREEN = '\033[92m'
RED = '\033[91m'
YELLOW = '\033[93m'
RESET = '\033[0m'
CYAN = '\033[96m'

def check_file_exists(path, description):
    if os.path.exists(path):
        size = os.path.getsize(path)
        print(f"{GREEN}âœ… {description} Mevcut ({size} bytes){RESET}")
        return True
    else:
        print(f"{RED}âŒ {description} BULUNAMADI! ({path}){RESET}")
        return False

def audit_log_file(project_path):
    log_path = project_path / ".chat_history.log"
    print(f"\n--- ğŸ“œ LOG DOSYASI KONTROLÃœ ({log_path.name}) ---")
    
    if check_file_exists(log_path, "Log DosyasÄ±"):
        try:
            with open(log_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                print(f"   ğŸ“„ Toplam SatÄ±r: {len(lines)}")
                if len(lines) > 0:
                    print(f"   ğŸ” Son KayÄ±t: {lines[-2].strip() if len(lines) > 1 else lines[0].strip()}")
                else:
                    print(f"{YELLOW}   âš ï¸ Dosya var ama iÃ§i boÅŸ.{RESET}")
        except Exception as e:
            print(f"{RED}   âŒ Dosya okuma hatasÄ±: {e}{RESET}")

def audit_vector_db(project_path):
    db_path = project_path / ".coder_memory"
    sqlite_file = db_path / "chroma.sqlite3"
    
    print(f"\n--- ğŸ§  VEKTÃ–R VERÄ°TABANI KONTROLÃœ ---")
    
    if not os.path.exists(db_path):
        print(f"{RED}âŒ .coder_memory klasÃ¶rÃ¼ yok (HafÄ±zasÄ±z Proje).{RESET}")
        return

    if check_file_exists(sqlite_file, "ChromaDB SQLite DosyasÄ±"):
        try:
            # ChromaDB kÃ¼tÃ¼phanesini kullanmadan direkt SQL ile bÃ¼tÃ¼nlÃ¼k testi
            conn = sqlite3.connect(sqlite_file)
            cursor = conn.cursor()
            
            # TablolarÄ± say
            cursor.execute("SELECT count(*) FROM sqlite_master WHERE type='table';")
            tables = cursor.fetchone()[0]
            print(f"   ğŸ“Š Tablo SayÄ±sÄ±: {tables}")
            
            # Embedding sayÄ±sÄ±nÄ± bulmaya Ã§alÄ±ÅŸ
            try:
                # Chroma versiyonuna gÃ¶re tablo adÄ± deÄŸiÅŸebilir, genelde 'embeddings'
                cursor.execute("SELECT count(*) FROM embeddings;")
                count = cursor.fetchone()[0]
                print(f"   ğŸ§¬ Ä°ndekslenmiÅŸ VektÃ¶r SayÄ±sÄ±: {GREEN}{count}{RESET}")
            except:
                print(f"{YELLOW}   âš ï¸ 'embeddings' tablosu direkt okunamadÄ± (Versiyon farkÄ± olabilir).{RESET}")
                
            conn.close()
            print(f"{GREEN}   âœ… VeritabanÄ± bÃ¼tÃ¼nlÃ¼ÄŸÃ¼ (Integrity) saÄŸlam.{RESET}")
            
        except Exception as e:
            print(f"{RED}   âŒ VeritabanÄ± bozuk veya okunamÄ±yor: {e}{RESET}")

def main():
    # DÃœZELTME: ArtÄ±k kÃ¶k dizine deÄŸil, my_projects iÃ§ine bakÄ±yoruz
    workspace = Path.cwd() / PROJECTS_DIR_NAME
    
    if not workspace.exists():
        print(f"{RED}Hata: '{PROJECTS_DIR_NAME}' klasÃ¶rÃ¼ bulunamadÄ±.{RESET}")
        return

    # Projeleri bul (iÃ§inde .coder_memory olan klasÃ¶rler)
    projects = [d for d in workspace.iterdir() if d.is_dir() and (d / ".coder_memory").exists()]
    
    print(f"{CYAN}ğŸ” SÄ°STEM DENETÃ‡Ä°SÄ° BAÅLATILDI{RESET}")
    print(f"ğŸ“‚ Hedef Dizin: {workspace}")
    
    if not projects:
        print(f"{RED}Test edilecek proje bulunamadÄ±.{RESET}")
        return

    print(f"âœ… {len(projects)} adet proje tespit edildi.")
    
    for proj in projects:
        print(f"\n{YELLOW}========================================{RESET}")
        print(f"ğŸ“‚ PROJE DENETLENÄ°YOR: {proj.name}")
        print(f"{YELLOW}========================================{RESET}")
        
        audit_log_file(proj)
        audit_vector_db(proj)

if __name__ == "__main__":
    main()
```

#### ğŸ“„ Dosya: `user_settings.json`

```json
{
    "active_profile": "medium"
}
```

#### ğŸ“„ Dosya: `core/base.py`

```py
# core/base.py: Ortak ArayÃ¼z ve Hata TanÄ±mlarÄ±

class ModelAPIError(Exception):
    """API baÄŸlantÄ±/kota hatalarÄ± iÃ§in genel hata sÄ±nÄ±fÄ±."""
    pass

class BaseModel:
    """TÃ¼m model sÄ±nÄ±flarÄ±nÄ±n miras alacaÄŸÄ± soyut sÄ±nÄ±f."""
    MODEL_NAME = "Temel Model"

    def __init__(self):
        # API anahtarÄ±nÄ± kontrol etme vb.
        pass

    def generate_content(self, system_instruction, prompt_text):
        """AI'dan iÃ§erik Ã¼retme Ã§aÄŸrÄ±sÄ±."""
        raise NotImplementedError("Bu metot alt sÄ±nÄ±flar tarafÄ±ndan uygulanmalÄ±dÄ±r.")
```

#### ğŸ“„ Dosya: `core/deepseek.py`

```py
# core/deepseek.py
import os
import requests
from .base import BaseModel, ModelAPIError
from config import MODEL_CONFIGS

class DeepSeekModel(BaseModel):
    """DeepSeek API - Ãœcretsiz ve gÃ¼Ã§lÃ¼"""
    
    def __init__(self):
        conf = MODEL_CONFIGS["deepseek"]
        self.MODEL_NAME = conf["display_name"]
        
        self.api_key = os.getenv(conf["env_var"])
        if not self.api_key:
            raise ModelAPIError(f"{conf['env_var']} ortam deÄŸiÅŸkeni bulunamadÄ±.")
        
        # DeepSeek OpenAI uyumlu API uÃ§ noktasÄ±
        self.api_url = "https://api.deepseek.com/v1/chat/completions"
        self.model_id = conf["model_id"]
    
    def generate_content(self, system_instruction, prompt_text):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model_id,
            "messages": [
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": prompt_text}
            ],
            "temperature": 0.1,
            "max_tokens": 8000,
            "response_format": {"type": "json_object"}  # JSON zorla
        }
        
        try:
            response = requests.post(self.api_url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            result = response.json()
            
            # Debug iÃ§in
            if hasattr(self, 'DEBUG') and self.DEBUG:
                print(f"DEBUG DeepSeek Response: {result}")
                
            return result["choices"][0]["message"]["content"].strip()
            
        except requests.exceptions.RequestException as e:
            # Hata mesajÄ±nÄ± daha detaylÄ± gÃ¶rmek iÃ§in
            if hasattr(e, 'response') and e.response is not None:
                error_msg = e.response.text
                print(f"DEBUG DeepSeek Error: {error_msg}")
                try:
                    error_json = json.loads(error_msg)
                    raise ModelAPIError(f"DeepSeek HatasÄ±: {error_json.get('message', str(e))}")
                except:
                    raise ModelAPIError(f"DeepSeek API HatasÄ±: {e}")
            else:
                raise ModelAPIError(f"DeepSeek BaÄŸlantÄ± HatasÄ±: {e}")
        except Exception as e:
            raise ModelAPIError(f"DeepSeek Ä°ÅŸlem HatasÄ±: {e}")
```

#### ğŸ“„ Dosya: `core/gemini.py`

```py
import os
from google import genai
from google.genai import types
from .base import BaseModel, ModelAPIError
from config import MODEL_CONFIGS

class GeminiModel(BaseModel):
    def __init__(self):
        conf = MODEL_CONFIGS["gemini"]
        self.MODEL_NAME = conf["display_name"]
        self.raw_model_name = conf["model_name"] # Fiyat hesaplamasÄ± iÃ§in
        
        api_key = os.getenv(conf["env_var"])
        if not api_key:
            raise ModelAPIError(f"{conf['env_var']} bulunamadÄ±.")

        try:
            self.client = genai.Client(api_key=api_key)
        except Exception as e:
            raise ModelAPIError(f"Gemini Client BaÅŸlatÄ±lamadÄ±: {e}")

    def generate_content(self, system_instruction, prompt_text):
        try:
            response = self.client.models.generate_content(
                model=self.raw_model_name,
                contents=[prompt_text],
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    temperature=0.1
                )
            )
            
            # Token kullanÄ±mÄ±nÄ± gÃ¼venli ÅŸekilde al
            usage = {
                "input_tokens": 0,
                "output_tokens": 0
            }
            
            if hasattr(response, 'usage_metadata'):
                usage["input_tokens"] = response.usage_metadata.prompt_token_count
                usage["output_tokens"] = response.usage_metadata.candidates_token_count

            return {
                "content": response.text.strip(),
                "usage": usage,
                "model_key": self.raw_model_name
            }

        except Exception as e:
            raise ModelAPIError(f"Gemini HatasÄ±: {e}")
```

#### ğŸ“„ Dosya: `core/groq.py`

```py
# core/groq.py (DOÄRU VERSÄ°YON)
import os
import requests
from .base import BaseModel, ModelAPIError
from config import MODEL_CONFIGS

class GroqModel(BaseModel):
    """Groq LPU - Ultra hÄ±zlÄ± inference"""
    
    def __init__(self):
        conf = MODEL_CONFIGS["groq"]
        self.MODEL_NAME = conf["display_name"]
        
        # âš ï¸ Buradaki atamalarÄ±n doÄŸru yapÄ±ldÄ±ÄŸÄ±ndan emin olun:
        self.api_key = os.getenv(conf["env_var"])
        if not self.api_key:
            raise ModelAPIError(f"{conf['env_var']} ortam deÄŸiÅŸkeni bulunamadÄ±.")
        
        self.api_url = "https://api.groq.com/openai/v1/chat/completions"
        self.model_id = conf["model_id"]
    
    def generate_content(self, system_instruction, prompt_text):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model_id,
            "messages": [
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": prompt_text}
            ],
            "temperature": 0.1,
            "max_tokens": 8000,
            "response_format": {"type": "json_object"}  # JSON zorla
        }
        
        try:
            response = requests.post(self.api_url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["message"]["content"].strip()
        except Exception as e:
            # Hata mesajÄ±nÄ± daha detaylÄ± gÃ¶rmek iÃ§in
            if hasattr(e, 'response') and e.response is not None:
                 print(f"DEBUG RESPONSE: {e.response.text}")
            raise ModelAPIError(f"Groq API HatasÄ±: {e}")
```

#### ğŸ“„ Dosya: `core/huggingface.py`

```py
# core/huggingface.py
import os
import requests
import json
from .base import BaseModel, ModelAPIError
from config import MODEL_CONFIGS

class HuggingFaceModel(BaseModel):
    def __init__(self):
        conf = MODEL_CONFIGS["huggingface"]
        self.MODEL_NAME = conf["display_name"]
        self.model_id = conf["model_id"]
        
        self.api_key = os.getenv(conf["env_var"])
        if not self.api_key:
            raise ModelAPIError(f"{conf['env_var']} bulunamadÄ±.")
        
        self.headers = {"Authorization": f"Bearer {self.api_key}"}
        self.api_url = f"https://router.huggingface.co/models/{self.model_id}"

    def generate_content(self, system_instruction, prompt_text):
        # --- PROMPT FORMATLAMA ---
        # Qwen ve modern modeller iÃ§in ChatML formatÄ± en iyisidir
        if "qwen" in self.model_id.lower():
            full_prompt = (
                f"<|im_start|>system\n{system_instruction}<|im_end|>\n"
                f"<|im_start|>user\n{prompt_text}<|im_end|>\n"
                f"<|im_start|>assistant\n"
            )
        elif "llama-3" in self.model_id.lower():
            full_prompt = (
                f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{system_instruction}<|eot_id|>\n"
                f"<|start_header_id|>user<|end_header_id|>\n{prompt_text}<|eot_id|>\n"
                f"<|start_header_id|>assistant<|end_header_id|>"
            )
        else:
            # VarsayÄ±lan (Mistral/Eski Llama)
            full_prompt = f"[INST] <<SYS>>\n{system_instruction}\n<</SYS>>\n{prompt_text} [/INST]"

        payload = {
            "inputs": full_prompt,
            "parameters": {
                "max_new_tokens": 4096, # Kod Ã¼retimi iÃ§in yÃ¼ksek token
                "temperature": 0.1,     # TutarlÄ±lÄ±k iÃ§in dÃ¼ÅŸÃ¼k sÄ±caklÄ±k
                "return_full_text": False
            }
        }

        try:
            response = requests.post(self.api_url, headers=self.headers, json=payload)
            response.raise_for_status()
            
            result = response.json()
            
            # Hugging Face API bazen liste, bazen dict dÃ¶ner
            if isinstance(result, list) and len(result) > 0:
                return result[0].get('generated_text', '').strip()
            elif isinstance(result, dict):
                return result.get('generated_text', '').strip()
            else:
                raise ModelAPIError(f"Beklenmeyen API yanÄ±t formatÄ±: {type(result)}")

        except Exception as e:
            raise ModelAPIError(f"HF API HatasÄ±: {e}")
```

#### ğŸ“„ Dosya: `core/memory.py`

```py
import os
import shutil
import chromadb
import torch
import json
import numpy as np
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
import config
from config import Colors

class MemoryManager:
    def __init__(self, project_root: str):
        self.project_root = project_root
        self.memory_path = os.path.join(project_root, config.MEMORY_DIR_NAME)
        self.bm25_path = os.path.join(self.memory_path, "keyword_index.json")
        
        # 1. DonanÄ±m AlgÄ±lama
        self.device = self._detect_device()
        print(f"{Colors.MAGENTA}ğŸ§  Hibrit HafÄ±za Motoru BaÅŸlatÄ±lÄ±yor... ({self.device}){Colors.RESET}")
        
        # 2. VektÃ¶r Motoru (ChromaDB)
        self.embedder = SentenceTransformer(config.EMBEDDING_MODEL, device=self.device)
        os.makedirs(self.memory_path, exist_ok=True)
        self.client = chromadb.PersistentClient(path=self.memory_path)
        self.collection = self.client.get_or_create_collection(
            name=config.COLLECTION_NAME,
            metadata={"hnsw:space": "cosine"}
        )

        # 3. Anahtar Kelime Motoru (BM25)
        self.bm25 = None
        self.indexed_files = []
        self._load_bm25()

    def _detect_device(self):
        if torch.cuda.is_available(): return "cuda"
        if torch.backends.mps.is_available(): return "mps"
        return "cpu"

    def _load_bm25(self):
        """BM25 indeksini diskten yÃ¼kler."""
        if os.path.exists(self.bm25_path):
            try:
                with open(self.bm25_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.indexed_files = data['files']
                    corpus = [doc.split() for doc in data['corpus']]
                    self.bm25 = BM25Okapi(corpus)
            except: pass

    def index_files(self, file_paths: list):
        """DosyalarÄ± hem VektÃ¶r hem de BM25 iÃ§in indeksler."""
        documents = []
        metadatas = []
        ids = []
        corpus_for_bm25 = []

        print(f"{Colors.CYAN}ğŸ“¥ {len(file_paths)} dosya hibrit indeksleniyor...{Colors.RESET}")

        for fpath in file_paths:
            full_path = os.path.join(self.project_root, fpath)
            if not os.path.exists(full_path): continue
            
            try:
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                if not content.strip(): continue

                documents.append(content)
                metadatas.append({"source": fpath})
                ids.append(fpath)
                corpus_for_bm25.append(content)
                if fpath not in self.indexed_files: self.indexed_files.append(fpath)

            except Exception as e:
                print(f"{Colors.YELLOW}UyarÄ±: {fpath} okunamadÄ± ({e}){Colors.RESET}")

        if documents:
            # VektÃ¶r KayÄ±t
            embeddings = self.embedder.encode(documents, normalize_embeddings=True).tolist()
            self.collection.upsert(documents=documents, embeddings=embeddings, metadatas=metadatas, ids=ids)
            
            # BM25 KayÄ±t
            current_data = {"files": self.indexed_files, "corpus": documents} 
            with open(self.bm25_path, 'w', encoding='utf-8') as f:
                json.dump(current_data, f)
            self._load_bm25()
            print(f"{Colors.GREEN}âœ… Hibrit hafÄ±za gÃ¼ncellendi.{Colors.RESET}")

    def query(self, prompt: str, n_results=config.MAX_CONTEXT_RESULTS):
        """Hibrit Arama: VektÃ¶r + BM25 sonuÃ§larÄ±nÄ± birleÅŸtirir."""
        # 1. VektÃ¶r AramasÄ± (Anlamsal)
        query_embedding = self.embedder.encode([prompt], normalize_embeddings=True).tolist()
        vector_results = self.collection.query(query_embeddings=query_embedding, n_results=n_results)
        
        vector_docs = []
        if vector_results['documents']:
            for i, doc in enumerate(vector_results['documents'][0]):
                source = vector_results['metadatas'][0][i]['source']
                vector_docs.append((source, doc, "VektÃ¶r"))

        # 2. BM25 AramasÄ± (Anahtar Kelime)
        bm25_docs = []
        if self.bm25:
            tokenized_query = prompt.split()
            top_n = self.bm25.get_top_n(tokenized_query, self.indexed_files, n=n_results)
            for source in top_n:
                # BM25'ten gelen dosyanÄ±n iÃ§eriÄŸini Chroma'dan Ã§ekelim
                res = self.collection.get(ids=[source])
                if res['documents']:
                    bm25_docs.append((source, res['documents'][0], "Keyword"))

        # 3. SonuÃ§larÄ± BirleÅŸtir (TekilleÅŸtir)
        seen_sources = set()
        final_context = []
        
        # Ã–ncelik: BM25 (Nokta atÄ±ÅŸÄ± kelime eÅŸleÅŸmesi) sonra VektÃ¶r
        for source, doc, mtype in (bm25_docs + vector_docs):
            if source not in seen_sources:
                final_context.append(f"--- BAÄLAM ({mtype}): {source} ---\n{doc}\n")
                seen_sources.add(source)
                if len(final_context) >= n_results: break
        
        return "\n".join(final_context)
```

#### ğŸ“„ Dosya: `core/orchestrator.py`

```py
# core/orchestrator.py
import json
from config import Colors, ARCHITECT_INSTRUCTION, DEVELOPER_INSTRUCTION
from core.groq import GroqModel
from core.gemini import GeminiModel

class AgentOrchestrator:
    def __init__(self):
        self.architect = GroqModel()  # HÄ±zlÄ± ve mantÄ±klÄ±
        self.developer = GeminiModel() # GeniÅŸ baÄŸlam ve hassas yazÄ±m

    def execute_workflow(self, prompt, context, working_dir):
        print(f"{Colors.MAGENTA}ğŸ—ï¸  MÄ°MAR (Groq) planÄ± hazÄ±rlÄ±yor...{Colors.RESET}")
        
        # 1. Mimar PlanÄ± Ã‡Ä±karÄ±r
        arch_prompt = f"BAÄLAM:\n{context}\n\nÄ°STEK: {prompt}"
        arch_res = self.architect.generate_content(ARCHITECT_INSTRUCTION, arch_prompt)
        
        # JSON temizleme ve yÃ¼kleme
        try:
            # Groq bazen string bazen dict dÃ¶nebilir, adaptÃ¶rÃ¼ne gÃ¶re ayarla
            plan_data = json.loads(arch_res) if isinstance(arch_res, str) else arch_res
        except:
            print(f"{Colors.RED}Mimar planÄ± oluÅŸturamadÄ±.{Colors.RESET}")
            return None

        print(f"\n{Colors.CYAN}ğŸ“‹ MÄ°MARIN PLANI:{Colors.RESET}\n{plan_data.get('plan')}")
        print(f"ğŸ“‚ Etkilenecek Dosyalar: {plan_data.get('etkilenecek_dosyalar')}")

        confirm = input(f"\n{Colors.YELLOW}Bu planÄ± onaylÄ±yor musunuz? (e/h): {Colors.RESET}").lower()
        if confirm != 'e':
            return None

        # 2. MÃ¼hendis Kodu Yazar
        print(f"\n{Colors.GREEN}ğŸ‘¨â€ğŸ’» MÃœHENDÄ°S (Gemini) kodlamaya baÅŸlÄ±yor...{Colors.RESET}")
        dev_prompt = f"MÄ°MAR PLANI: {plan_data.get('plan')}\n\nBAÄLAM: {context}\n\nÄ°STEK: {prompt}"
        dev_res = self.developer.generate_content(DEVELOPER_INSTRUCTION, dev_prompt)
        
        return dev_res # Assistant.py'deki clean_json_string'e gidecek
```
